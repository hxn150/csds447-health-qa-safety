{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 5090,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003929273084479371,
      "grad_norm": 1.01180100440979,
      "learning_rate": 1.9976424361493126e-05,
      "loss": 1.9452,
      "step": 10
    },
    {
      "epoch": 0.007858546168958742,
      "grad_norm": 0.900098979473114,
      "learning_rate": 1.9950229207596598e-05,
      "loss": 1.9074,
      "step": 20
    },
    {
      "epoch": 0.011787819253438114,
      "grad_norm": 1.0589885711669922,
      "learning_rate": 1.9924034053700067e-05,
      "loss": 1.8632,
      "step": 30
    },
    {
      "epoch": 0.015717092337917484,
      "grad_norm": 0.9151074290275574,
      "learning_rate": 1.989783889980354e-05,
      "loss": 1.7727,
      "step": 40
    },
    {
      "epoch": 0.019646365422396856,
      "grad_norm": 1.1239510774612427,
      "learning_rate": 1.9871643745907007e-05,
      "loss": 1.7637,
      "step": 50
    },
    {
      "epoch": 0.023575638506876228,
      "grad_norm": 1.4985692501068115,
      "learning_rate": 1.984544859201048e-05,
      "loss": 1.7182,
      "step": 60
    },
    {
      "epoch": 0.0275049115913556,
      "grad_norm": 1.2924318313598633,
      "learning_rate": 1.981925343811395e-05,
      "loss": 1.72,
      "step": 70
    },
    {
      "epoch": 0.03143418467583497,
      "grad_norm": 1.2369296550750732,
      "learning_rate": 1.979305828421742e-05,
      "loss": 1.6236,
      "step": 80
    },
    {
      "epoch": 0.03536345776031434,
      "grad_norm": 1.4362916946411133,
      "learning_rate": 1.9766863130320892e-05,
      "loss": 1.5766,
      "step": 90
    },
    {
      "epoch": 0.03929273084479371,
      "grad_norm": 1.4943069219589233,
      "learning_rate": 1.9740667976424364e-05,
      "loss": 1.4949,
      "step": 100
    },
    {
      "epoch": 0.043222003929273084,
      "grad_norm": 1.7443634271621704,
      "learning_rate": 1.9714472822527836e-05,
      "loss": 1.4531,
      "step": 110
    },
    {
      "epoch": 0.047151277013752456,
      "grad_norm": 1.5360010862350464,
      "learning_rate": 1.9688277668631305e-05,
      "loss": 1.4094,
      "step": 120
    },
    {
      "epoch": 0.05108055009823183,
      "grad_norm": 1.4274359941482544,
      "learning_rate": 1.9662082514734777e-05,
      "loss": 1.3555,
      "step": 130
    },
    {
      "epoch": 0.0550098231827112,
      "grad_norm": 1.3133782148361206,
      "learning_rate": 1.9635887360838245e-05,
      "loss": 1.3279,
      "step": 140
    },
    {
      "epoch": 0.05893909626719057,
      "grad_norm": 1.3449288606643677,
      "learning_rate": 1.9609692206941717e-05,
      "loss": 1.3518,
      "step": 150
    },
    {
      "epoch": 0.06286836935166994,
      "grad_norm": 1.0633208751678467,
      "learning_rate": 1.958349705304519e-05,
      "loss": 1.2388,
      "step": 160
    },
    {
      "epoch": 0.06679764243614932,
      "grad_norm": 0.9483070373535156,
      "learning_rate": 1.955730189914866e-05,
      "loss": 1.2499,
      "step": 170
    },
    {
      "epoch": 0.07072691552062868,
      "grad_norm": 1.1553581953048706,
      "learning_rate": 1.953110674525213e-05,
      "loss": 1.2284,
      "step": 180
    },
    {
      "epoch": 0.07465618860510806,
      "grad_norm": 1.073641300201416,
      "learning_rate": 1.95049115913556e-05,
      "loss": 1.1755,
      "step": 190
    },
    {
      "epoch": 0.07858546168958742,
      "grad_norm": 1.0942778587341309,
      "learning_rate": 1.947871643745907e-05,
      "loss": 1.2232,
      "step": 200
    },
    {
      "epoch": 0.0825147347740668,
      "grad_norm": 1.1121456623077393,
      "learning_rate": 1.9452521283562542e-05,
      "loss": 1.2222,
      "step": 210
    },
    {
      "epoch": 0.08644400785854617,
      "grad_norm": 1.0751787424087524,
      "learning_rate": 1.9426326129666014e-05,
      "loss": 1.2429,
      "step": 220
    },
    {
      "epoch": 0.09037328094302555,
      "grad_norm": 1.016428828239441,
      "learning_rate": 1.9400130975769483e-05,
      "loss": 1.2063,
      "step": 230
    },
    {
      "epoch": 0.09430255402750491,
      "grad_norm": 1.2008495330810547,
      "learning_rate": 1.9373935821872955e-05,
      "loss": 1.2202,
      "step": 240
    },
    {
      "epoch": 0.09823182711198428,
      "grad_norm": 1.3994643688201904,
      "learning_rate": 1.9347740667976427e-05,
      "loss": 1.16,
      "step": 250
    },
    {
      "epoch": 0.10216110019646366,
      "grad_norm": 1.089542031288147,
      "learning_rate": 1.9321545514079896e-05,
      "loss": 1.2236,
      "step": 260
    },
    {
      "epoch": 0.10609037328094302,
      "grad_norm": 1.1375031471252441,
      "learning_rate": 1.9295350360183368e-05,
      "loss": 1.2399,
      "step": 270
    },
    {
      "epoch": 0.1100196463654224,
      "grad_norm": 1.0683540105819702,
      "learning_rate": 1.926915520628684e-05,
      "loss": 1.2013,
      "step": 280
    },
    {
      "epoch": 0.11394891944990176,
      "grad_norm": 1.0369712114334106,
      "learning_rate": 1.9242960052390308e-05,
      "loss": 1.1791,
      "step": 290
    },
    {
      "epoch": 0.11787819253438114,
      "grad_norm": 1.1129213571548462,
      "learning_rate": 1.921676489849378e-05,
      "loss": 1.2142,
      "step": 300
    },
    {
      "epoch": 0.12180746561886051,
      "grad_norm": 1.2687281370162964,
      "learning_rate": 1.9190569744597252e-05,
      "loss": 1.223,
      "step": 310
    },
    {
      "epoch": 0.12573673870333987,
      "grad_norm": 1.4741055965423584,
      "learning_rate": 1.9164374590700724e-05,
      "loss": 1.1556,
      "step": 320
    },
    {
      "epoch": 0.12966601178781925,
      "grad_norm": 1.1414908170700073,
      "learning_rate": 1.9138179436804193e-05,
      "loss": 1.2253,
      "step": 330
    },
    {
      "epoch": 0.13359528487229863,
      "grad_norm": 1.2080167531967163,
      "learning_rate": 1.911198428290766e-05,
      "loss": 1.2155,
      "step": 340
    },
    {
      "epoch": 0.137524557956778,
      "grad_norm": 1.261121153831482,
      "learning_rate": 1.9085789129011134e-05,
      "loss": 1.1909,
      "step": 350
    },
    {
      "epoch": 0.14145383104125736,
      "grad_norm": 1.1528559923171997,
      "learning_rate": 1.9059593975114606e-05,
      "loss": 1.1891,
      "step": 360
    },
    {
      "epoch": 0.14538310412573674,
      "grad_norm": 1.1830289363861084,
      "learning_rate": 1.9033398821218078e-05,
      "loss": 1.2137,
      "step": 370
    },
    {
      "epoch": 0.14931237721021612,
      "grad_norm": 1.1750404834747314,
      "learning_rate": 1.9007203667321546e-05,
      "loss": 1.2112,
      "step": 380
    },
    {
      "epoch": 0.15324165029469547,
      "grad_norm": 1.180430293083191,
      "learning_rate": 1.8981008513425018e-05,
      "loss": 1.1974,
      "step": 390
    },
    {
      "epoch": 0.15717092337917485,
      "grad_norm": 1.1203218698501587,
      "learning_rate": 1.8954813359528487e-05,
      "loss": 1.2153,
      "step": 400
    },
    {
      "epoch": 0.16110019646365423,
      "grad_norm": 1.8639001846313477,
      "learning_rate": 1.892861820563196e-05,
      "loss": 1.1897,
      "step": 410
    },
    {
      "epoch": 0.1650294695481336,
      "grad_norm": 1.2140908241271973,
      "learning_rate": 1.890242305173543e-05,
      "loss": 1.1846,
      "step": 420
    },
    {
      "epoch": 0.16895874263261296,
      "grad_norm": 1.3493142127990723,
      "learning_rate": 1.8876227897838903e-05,
      "loss": 1.1656,
      "step": 430
    },
    {
      "epoch": 0.17288801571709234,
      "grad_norm": 1.483255386352539,
      "learning_rate": 1.885003274394237e-05,
      "loss": 1.2022,
      "step": 440
    },
    {
      "epoch": 0.17681728880157171,
      "grad_norm": 1.3962702751159668,
      "learning_rate": 1.8823837590045843e-05,
      "loss": 1.1995,
      "step": 450
    },
    {
      "epoch": 0.1807465618860511,
      "grad_norm": 1.3474321365356445,
      "learning_rate": 1.8797642436149315e-05,
      "loss": 1.2046,
      "step": 460
    },
    {
      "epoch": 0.18467583497053044,
      "grad_norm": 1.2273519039154053,
      "learning_rate": 1.8771447282252784e-05,
      "loss": 1.198,
      "step": 470
    },
    {
      "epoch": 0.18860510805500982,
      "grad_norm": 1.4015517234802246,
      "learning_rate": 1.8745252128356256e-05,
      "loss": 1.2221,
      "step": 480
    },
    {
      "epoch": 0.1925343811394892,
      "grad_norm": 1.2428395748138428,
      "learning_rate": 1.8719056974459725e-05,
      "loss": 1.1813,
      "step": 490
    },
    {
      "epoch": 0.19646365422396855,
      "grad_norm": 1.3151137828826904,
      "learning_rate": 1.8692861820563197e-05,
      "loss": 1.2038,
      "step": 500
    },
    {
      "epoch": 0.20039292730844793,
      "grad_norm": 1.2519347667694092,
      "learning_rate": 1.866666666666667e-05,
      "loss": 1.2236,
      "step": 510
    },
    {
      "epoch": 0.2043222003929273,
      "grad_norm": 1.5654246807098389,
      "learning_rate": 1.864047151277014e-05,
      "loss": 1.1591,
      "step": 520
    },
    {
      "epoch": 0.2082514734774067,
      "grad_norm": 1.6421881914138794,
      "learning_rate": 1.861427635887361e-05,
      "loss": 1.1763,
      "step": 530
    },
    {
      "epoch": 0.21218074656188604,
      "grad_norm": 1.4952456951141357,
      "learning_rate": 1.858808120497708e-05,
      "loss": 1.1452,
      "step": 540
    },
    {
      "epoch": 0.21611001964636542,
      "grad_norm": 1.4002479314804077,
      "learning_rate": 1.856188605108055e-05,
      "loss": 1.099,
      "step": 550
    },
    {
      "epoch": 0.2200392927308448,
      "grad_norm": 1.3622260093688965,
      "learning_rate": 1.8535690897184022e-05,
      "loss": 1.1987,
      "step": 560
    },
    {
      "epoch": 0.22396856581532418,
      "grad_norm": 1.2466155290603638,
      "learning_rate": 1.8509495743287494e-05,
      "loss": 1.1764,
      "step": 570
    },
    {
      "epoch": 0.22789783889980353,
      "grad_norm": 1.2087730169296265,
      "learning_rate": 1.8483300589390966e-05,
      "loss": 1.1949,
      "step": 580
    },
    {
      "epoch": 0.2318271119842829,
      "grad_norm": 1.2908552885055542,
      "learning_rate": 1.8457105435494435e-05,
      "loss": 1.2148,
      "step": 590
    },
    {
      "epoch": 0.2357563850687623,
      "grad_norm": 1.2775119543075562,
      "learning_rate": 1.8430910281597907e-05,
      "loss": 1.154,
      "step": 600
    },
    {
      "epoch": 0.23968565815324164,
      "grad_norm": 1.627360463142395,
      "learning_rate": 1.8404715127701375e-05,
      "loss": 1.1801,
      "step": 610
    },
    {
      "epoch": 0.24361493123772102,
      "grad_norm": 1.3416224718093872,
      "learning_rate": 1.8378519973804847e-05,
      "loss": 1.1861,
      "step": 620
    },
    {
      "epoch": 0.2475442043222004,
      "grad_norm": 1.5978626012802124,
      "learning_rate": 1.835232481990832e-05,
      "loss": 1.1508,
      "step": 630
    },
    {
      "epoch": 0.25147347740667975,
      "grad_norm": 1.6803970336914062,
      "learning_rate": 1.8326129666011788e-05,
      "loss": 1.1836,
      "step": 640
    },
    {
      "epoch": 0.2554027504911591,
      "grad_norm": 1.3534297943115234,
      "learning_rate": 1.829993451211526e-05,
      "loss": 1.1587,
      "step": 650
    },
    {
      "epoch": 0.2593320235756385,
      "grad_norm": 1.4025570154190063,
      "learning_rate": 1.8273739358218732e-05,
      "loss": 1.177,
      "step": 660
    },
    {
      "epoch": 0.2632612966601179,
      "grad_norm": 1.5666484832763672,
      "learning_rate": 1.8247544204322204e-05,
      "loss": 1.2104,
      "step": 670
    },
    {
      "epoch": 0.26719056974459726,
      "grad_norm": 1.3346914052963257,
      "learning_rate": 1.8221349050425672e-05,
      "loss": 1.1717,
      "step": 680
    },
    {
      "epoch": 0.27111984282907664,
      "grad_norm": 1.5060111284255981,
      "learning_rate": 1.8195153896529144e-05,
      "loss": 1.1089,
      "step": 690
    },
    {
      "epoch": 0.275049115913556,
      "grad_norm": 1.2215341329574585,
      "learning_rate": 1.8168958742632613e-05,
      "loss": 1.1331,
      "step": 700
    },
    {
      "epoch": 0.27897838899803534,
      "grad_norm": 1.5008281469345093,
      "learning_rate": 1.8142763588736085e-05,
      "loss": 1.166,
      "step": 710
    },
    {
      "epoch": 0.2829076620825147,
      "grad_norm": 1.551487684249878,
      "learning_rate": 1.8116568434839557e-05,
      "loss": 1.1609,
      "step": 720
    },
    {
      "epoch": 0.2868369351669941,
      "grad_norm": 1.1901432275772095,
      "learning_rate": 1.809037328094303e-05,
      "loss": 1.164,
      "step": 730
    },
    {
      "epoch": 0.2907662082514735,
      "grad_norm": 1.2042639255523682,
      "learning_rate": 1.8064178127046498e-05,
      "loss": 1.2179,
      "step": 740
    },
    {
      "epoch": 0.29469548133595286,
      "grad_norm": 1.4471495151519775,
      "learning_rate": 1.8037982973149966e-05,
      "loss": 1.1894,
      "step": 750
    },
    {
      "epoch": 0.29862475442043224,
      "grad_norm": 1.325324296951294,
      "learning_rate": 1.8011787819253438e-05,
      "loss": 1.1815,
      "step": 760
    },
    {
      "epoch": 0.3025540275049116,
      "grad_norm": 1.3705836534500122,
      "learning_rate": 1.798559266535691e-05,
      "loss": 1.2083,
      "step": 770
    },
    {
      "epoch": 0.30648330058939094,
      "grad_norm": 1.523690938949585,
      "learning_rate": 1.7959397511460382e-05,
      "loss": 1.1829,
      "step": 780
    },
    {
      "epoch": 0.3104125736738703,
      "grad_norm": 1.3721814155578613,
      "learning_rate": 1.793320235756385e-05,
      "loss": 1.135,
      "step": 790
    },
    {
      "epoch": 0.3143418467583497,
      "grad_norm": 1.2780081033706665,
      "learning_rate": 1.7907007203667323e-05,
      "loss": 1.2069,
      "step": 800
    },
    {
      "epoch": 0.3182711198428291,
      "grad_norm": 1.4126454591751099,
      "learning_rate": 1.7880812049770795e-05,
      "loss": 1.1463,
      "step": 810
    },
    {
      "epoch": 0.32220039292730845,
      "grad_norm": 1.2717198133468628,
      "learning_rate": 1.7854616895874263e-05,
      "loss": 1.0919,
      "step": 820
    },
    {
      "epoch": 0.32612966601178783,
      "grad_norm": 1.331737756729126,
      "learning_rate": 1.7828421741977735e-05,
      "loss": 1.1521,
      "step": 830
    },
    {
      "epoch": 0.3300589390962672,
      "grad_norm": 1.4879697561264038,
      "learning_rate": 1.7802226588081207e-05,
      "loss": 1.1603,
      "step": 840
    },
    {
      "epoch": 0.33398821218074654,
      "grad_norm": 1.4269334077835083,
      "learning_rate": 1.7776031434184676e-05,
      "loss": 1.1444,
      "step": 850
    },
    {
      "epoch": 0.3379174852652259,
      "grad_norm": 1.5801292657852173,
      "learning_rate": 1.7749836280288148e-05,
      "loss": 1.1173,
      "step": 860
    },
    {
      "epoch": 0.3418467583497053,
      "grad_norm": 1.5451279878616333,
      "learning_rate": 1.772364112639162e-05,
      "loss": 1.1667,
      "step": 870
    },
    {
      "epoch": 0.34577603143418467,
      "grad_norm": 1.3570278882980347,
      "learning_rate": 1.7697445972495092e-05,
      "loss": 1.216,
      "step": 880
    },
    {
      "epoch": 0.34970530451866405,
      "grad_norm": 1.4827466011047363,
      "learning_rate": 1.767125081859856e-05,
      "loss": 1.1894,
      "step": 890
    },
    {
      "epoch": 0.35363457760314343,
      "grad_norm": 1.366895318031311,
      "learning_rate": 1.764505566470203e-05,
      "loss": 1.1958,
      "step": 900
    },
    {
      "epoch": 0.3575638506876228,
      "grad_norm": 1.5189523696899414,
      "learning_rate": 1.76188605108055e-05,
      "loss": 1.1832,
      "step": 910
    },
    {
      "epoch": 0.3614931237721022,
      "grad_norm": 1.8953222036361694,
      "learning_rate": 1.7592665356908973e-05,
      "loss": 1.2105,
      "step": 920
    },
    {
      "epoch": 0.3654223968565815,
      "grad_norm": 1.4089092016220093,
      "learning_rate": 1.7566470203012445e-05,
      "loss": 1.1408,
      "step": 930
    },
    {
      "epoch": 0.3693516699410609,
      "grad_norm": 1.5848052501678467,
      "learning_rate": 1.7540275049115917e-05,
      "loss": 1.171,
      "step": 940
    },
    {
      "epoch": 0.37328094302554027,
      "grad_norm": 1.7273812294006348,
      "learning_rate": 1.7514079895219386e-05,
      "loss": 1.1645,
      "step": 950
    },
    {
      "epoch": 0.37721021611001965,
      "grad_norm": 1.380066156387329,
      "learning_rate": 1.7487884741322855e-05,
      "loss": 1.156,
      "step": 960
    },
    {
      "epoch": 0.381139489194499,
      "grad_norm": 1.5067315101623535,
      "learning_rate": 1.7461689587426327e-05,
      "loss": 1.1624,
      "step": 970
    },
    {
      "epoch": 0.3850687622789784,
      "grad_norm": 1.6098741292953491,
      "learning_rate": 1.74354944335298e-05,
      "loss": 1.1162,
      "step": 980
    },
    {
      "epoch": 0.3889980353634578,
      "grad_norm": 1.7521673440933228,
      "learning_rate": 1.740929927963327e-05,
      "loss": 1.1973,
      "step": 990
    },
    {
      "epoch": 0.3929273084479371,
      "grad_norm": 1.4272648096084595,
      "learning_rate": 1.738310412573674e-05,
      "loss": 1.174,
      "step": 1000
    },
    {
      "epoch": 0.3968565815324165,
      "grad_norm": 1.7578762769699097,
      "learning_rate": 1.735690897184021e-05,
      "loss": 1.1246,
      "step": 1010
    },
    {
      "epoch": 0.40078585461689586,
      "grad_norm": 1.3737527132034302,
      "learning_rate": 1.7330713817943683e-05,
      "loss": 1.1936,
      "step": 1020
    },
    {
      "epoch": 0.40471512770137524,
      "grad_norm": 1.3472362756729126,
      "learning_rate": 1.7304518664047152e-05,
      "loss": 1.1599,
      "step": 1030
    },
    {
      "epoch": 0.4086444007858546,
      "grad_norm": 1.5363647937774658,
      "learning_rate": 1.7278323510150624e-05,
      "loss": 1.1231,
      "step": 1040
    },
    {
      "epoch": 0.412573673870334,
      "grad_norm": 1.4436737298965454,
      "learning_rate": 1.7252128356254096e-05,
      "loss": 1.2409,
      "step": 1050
    },
    {
      "epoch": 0.4165029469548134,
      "grad_norm": 1.5471978187561035,
      "learning_rate": 1.7225933202357564e-05,
      "loss": 1.1312,
      "step": 1060
    },
    {
      "epoch": 0.4204322200392927,
      "grad_norm": 1.3460683822631836,
      "learning_rate": 1.7199738048461036e-05,
      "loss": 1.1592,
      "step": 1070
    },
    {
      "epoch": 0.4243614931237721,
      "grad_norm": 1.7245876789093018,
      "learning_rate": 1.717354289456451e-05,
      "loss": 1.1397,
      "step": 1080
    },
    {
      "epoch": 0.42829076620825146,
      "grad_norm": 1.5171171426773071,
      "learning_rate": 1.714734774066798e-05,
      "loss": 1.1786,
      "step": 1090
    },
    {
      "epoch": 0.43222003929273084,
      "grad_norm": 1.698542594909668,
      "learning_rate": 1.712115258677145e-05,
      "loss": 1.1715,
      "step": 1100
    },
    {
      "epoch": 0.4361493123772102,
      "grad_norm": 1.3406668901443481,
      "learning_rate": 1.7094957432874918e-05,
      "loss": 1.1612,
      "step": 1110
    },
    {
      "epoch": 0.4400785854616896,
      "grad_norm": 1.6712775230407715,
      "learning_rate": 1.706876227897839e-05,
      "loss": 1.1364,
      "step": 1120
    },
    {
      "epoch": 0.444007858546169,
      "grad_norm": 1.3847558498382568,
      "learning_rate": 1.704256712508186e-05,
      "loss": 1.1497,
      "step": 1130
    },
    {
      "epoch": 0.44793713163064836,
      "grad_norm": 1.6355326175689697,
      "learning_rate": 1.7016371971185334e-05,
      "loss": 1.1396,
      "step": 1140
    },
    {
      "epoch": 0.4518664047151277,
      "grad_norm": 1.3562629222869873,
      "learning_rate": 1.6990176817288802e-05,
      "loss": 1.1185,
      "step": 1150
    },
    {
      "epoch": 0.45579567779960706,
      "grad_norm": 1.5556550025939941,
      "learning_rate": 1.6963981663392274e-05,
      "loss": 1.1916,
      "step": 1160
    },
    {
      "epoch": 0.45972495088408644,
      "grad_norm": 1.5057965517044067,
      "learning_rate": 1.6937786509495743e-05,
      "loss": 1.1593,
      "step": 1170
    },
    {
      "epoch": 0.4636542239685658,
      "grad_norm": 1.6691159009933472,
      "learning_rate": 1.6911591355599215e-05,
      "loss": 1.1295,
      "step": 1180
    },
    {
      "epoch": 0.4675834970530452,
      "grad_norm": 1.822930932044983,
      "learning_rate": 1.6885396201702687e-05,
      "loss": 1.1646,
      "step": 1190
    },
    {
      "epoch": 0.4715127701375246,
      "grad_norm": 1.5147054195404053,
      "learning_rate": 1.685920104780616e-05,
      "loss": 1.1866,
      "step": 1200
    },
    {
      "epoch": 0.47544204322200395,
      "grad_norm": 1.4857852458953857,
      "learning_rate": 1.6833005893909628e-05,
      "loss": 1.1926,
      "step": 1210
    },
    {
      "epoch": 0.4793713163064833,
      "grad_norm": 1.4026038646697998,
      "learning_rate": 1.68068107400131e-05,
      "loss": 1.1385,
      "step": 1220
    },
    {
      "epoch": 0.48330058939096265,
      "grad_norm": 1.4475886821746826,
      "learning_rate": 1.6780615586116568e-05,
      "loss": 1.1988,
      "step": 1230
    },
    {
      "epoch": 0.48722986247544203,
      "grad_norm": 1.5030518770217896,
      "learning_rate": 1.675442043222004e-05,
      "loss": 1.1118,
      "step": 1240
    },
    {
      "epoch": 0.4911591355599214,
      "grad_norm": 1.5069472789764404,
      "learning_rate": 1.6728225278323512e-05,
      "loss": 1.1348,
      "step": 1250
    },
    {
      "epoch": 0.4950884086444008,
      "grad_norm": 2.2437872886657715,
      "learning_rate": 1.670203012442698e-05,
      "loss": 1.1432,
      "step": 1260
    },
    {
      "epoch": 0.49901768172888017,
      "grad_norm": 2.0109288692474365,
      "learning_rate": 1.6675834970530453e-05,
      "loss": 1.1679,
      "step": 1270
    },
    {
      "epoch": 0.5029469548133595,
      "grad_norm": 1.484266757965088,
      "learning_rate": 1.6649639816633925e-05,
      "loss": 1.1388,
      "step": 1280
    },
    {
      "epoch": 0.5068762278978389,
      "grad_norm": 1.4476052522659302,
      "learning_rate": 1.6623444662737397e-05,
      "loss": 1.1116,
      "step": 1290
    },
    {
      "epoch": 0.5108055009823183,
      "grad_norm": 1.6846672296524048,
      "learning_rate": 1.6597249508840865e-05,
      "loss": 1.156,
      "step": 1300
    },
    {
      "epoch": 0.5147347740667977,
      "grad_norm": 1.7727125883102417,
      "learning_rate": 1.6571054354944337e-05,
      "loss": 1.1534,
      "step": 1310
    },
    {
      "epoch": 0.518664047151277,
      "grad_norm": 1.656265139579773,
      "learning_rate": 1.6544859201047806e-05,
      "loss": 1.2087,
      "step": 1320
    },
    {
      "epoch": 0.5225933202357563,
      "grad_norm": 1.8292900323867798,
      "learning_rate": 1.6518664047151278e-05,
      "loss": 1.1547,
      "step": 1330
    },
    {
      "epoch": 0.5265225933202358,
      "grad_norm": 1.5906386375427246,
      "learning_rate": 1.649246889325475e-05,
      "loss": 1.1839,
      "step": 1340
    },
    {
      "epoch": 0.5304518664047151,
      "grad_norm": 1.8148566484451294,
      "learning_rate": 1.6466273739358222e-05,
      "loss": 1.1611,
      "step": 1350
    },
    {
      "epoch": 0.5343811394891945,
      "grad_norm": 1.854331374168396,
      "learning_rate": 1.644007858546169e-05,
      "loss": 1.1197,
      "step": 1360
    },
    {
      "epoch": 0.5383104125736738,
      "grad_norm": 1.4402027130126953,
      "learning_rate": 1.641388343156516e-05,
      "loss": 1.1352,
      "step": 1370
    },
    {
      "epoch": 0.5422396856581533,
      "grad_norm": 1.8374853134155273,
      "learning_rate": 1.638768827766863e-05,
      "loss": 1.1954,
      "step": 1380
    },
    {
      "epoch": 0.5461689587426326,
      "grad_norm": 1.8985549211502075,
      "learning_rate": 1.6361493123772103e-05,
      "loss": 1.1505,
      "step": 1390
    },
    {
      "epoch": 0.550098231827112,
      "grad_norm": 1.7971640825271606,
      "learning_rate": 1.6335297969875575e-05,
      "loss": 1.1386,
      "step": 1400
    },
    {
      "epoch": 0.5540275049115914,
      "grad_norm": 1.6678794622421265,
      "learning_rate": 1.6309102815979044e-05,
      "loss": 1.06,
      "step": 1410
    },
    {
      "epoch": 0.5579567779960707,
      "grad_norm": 1.6214286088943481,
      "learning_rate": 1.6282907662082516e-05,
      "loss": 1.1636,
      "step": 1420
    },
    {
      "epoch": 0.5618860510805501,
      "grad_norm": 2.0730340480804443,
      "learning_rate": 1.6256712508185988e-05,
      "loss": 1.1359,
      "step": 1430
    },
    {
      "epoch": 0.5658153241650294,
      "grad_norm": 1.8592305183410645,
      "learning_rate": 1.6230517354289456e-05,
      "loss": 1.1602,
      "step": 1440
    },
    {
      "epoch": 0.5697445972495089,
      "grad_norm": 1.5690175294876099,
      "learning_rate": 1.620432220039293e-05,
      "loss": 1.1578,
      "step": 1450
    },
    {
      "epoch": 0.5736738703339882,
      "grad_norm": 1.4550106525421143,
      "learning_rate": 1.61781270464964e-05,
      "loss": 1.1955,
      "step": 1460
    },
    {
      "epoch": 0.5776031434184676,
      "grad_norm": 1.4589526653289795,
      "learning_rate": 1.615193189259987e-05,
      "loss": 1.1406,
      "step": 1470
    },
    {
      "epoch": 0.581532416502947,
      "grad_norm": 2.5053133964538574,
      "learning_rate": 1.612573673870334e-05,
      "loss": 1.1547,
      "step": 1480
    },
    {
      "epoch": 0.5854616895874263,
      "grad_norm": 1.7750285863876343,
      "learning_rate": 1.6099541584806813e-05,
      "loss": 1.148,
      "step": 1490
    },
    {
      "epoch": 0.5893909626719057,
      "grad_norm": 1.8057523965835571,
      "learning_rate": 1.6073346430910285e-05,
      "loss": 1.0975,
      "step": 1500
    },
    {
      "epoch": 0.593320235756385,
      "grad_norm": 1.8098031282424927,
      "learning_rate": 1.6047151277013754e-05,
      "loss": 1.1286,
      "step": 1510
    },
    {
      "epoch": 0.5972495088408645,
      "grad_norm": 1.4478622674942017,
      "learning_rate": 1.6020956123117222e-05,
      "loss": 1.232,
      "step": 1520
    },
    {
      "epoch": 0.6011787819253438,
      "grad_norm": 1.8509339094161987,
      "learning_rate": 1.5994760969220694e-05,
      "loss": 1.1537,
      "step": 1530
    },
    {
      "epoch": 0.6051080550098232,
      "grad_norm": 1.7413309812545776,
      "learning_rate": 1.5968565815324166e-05,
      "loss": 1.2096,
      "step": 1540
    },
    {
      "epoch": 0.6090373280943026,
      "grad_norm": 1.8954318761825562,
      "learning_rate": 1.594237066142764e-05,
      "loss": 1.1925,
      "step": 1550
    },
    {
      "epoch": 0.6129666011787819,
      "grad_norm": 1.6050862073898315,
      "learning_rate": 1.5916175507531107e-05,
      "loss": 1.2062,
      "step": 1560
    },
    {
      "epoch": 0.6168958742632613,
      "grad_norm": 1.4801137447357178,
      "learning_rate": 1.588998035363458e-05,
      "loss": 1.2209,
      "step": 1570
    },
    {
      "epoch": 0.6208251473477406,
      "grad_norm": 1.8542598485946655,
      "learning_rate": 1.5863785199738048e-05,
      "loss": 1.1528,
      "step": 1580
    },
    {
      "epoch": 0.6247544204322201,
      "grad_norm": 1.4507213830947876,
      "learning_rate": 1.583759004584152e-05,
      "loss": 1.1333,
      "step": 1590
    },
    {
      "epoch": 0.6286836935166994,
      "grad_norm": 1.7146388292312622,
      "learning_rate": 1.581139489194499e-05,
      "loss": 1.1527,
      "step": 1600
    },
    {
      "epoch": 0.6326129666011788,
      "grad_norm": 2.812718152999878,
      "learning_rate": 1.5785199738048464e-05,
      "loss": 1.1593,
      "step": 1610
    },
    {
      "epoch": 0.6365422396856582,
      "grad_norm": 1.4491233825683594,
      "learning_rate": 1.5759004584151932e-05,
      "loss": 1.1259,
      "step": 1620
    },
    {
      "epoch": 0.6404715127701375,
      "grad_norm": 2.1092350482940674,
      "learning_rate": 1.5732809430255404e-05,
      "loss": 1.1958,
      "step": 1630
    },
    {
      "epoch": 0.6444007858546169,
      "grad_norm": 1.5050621032714844,
      "learning_rate": 1.5706614276358876e-05,
      "loss": 1.1474,
      "step": 1640
    },
    {
      "epoch": 0.6483300589390962,
      "grad_norm": 1.6423879861831665,
      "learning_rate": 1.5680419122462345e-05,
      "loss": 1.1467,
      "step": 1650
    },
    {
      "epoch": 0.6522593320235757,
      "grad_norm": 2.1333870887756348,
      "learning_rate": 1.5654223968565817e-05,
      "loss": 1.1595,
      "step": 1660
    },
    {
      "epoch": 0.656188605108055,
      "grad_norm": 1.8922452926635742,
      "learning_rate": 1.5628028814669285e-05,
      "loss": 1.1291,
      "step": 1670
    },
    {
      "epoch": 0.6601178781925344,
      "grad_norm": 1.9578064680099487,
      "learning_rate": 1.5601833660772757e-05,
      "loss": 1.0804,
      "step": 1680
    },
    {
      "epoch": 0.6640471512770137,
      "grad_norm": 1.5700700283050537,
      "learning_rate": 1.557563850687623e-05,
      "loss": 1.1505,
      "step": 1690
    },
    {
      "epoch": 0.6679764243614931,
      "grad_norm": 1.5737392902374268,
      "learning_rate": 1.55494433529797e-05,
      "loss": 1.1127,
      "step": 1700
    },
    {
      "epoch": 0.6719056974459725,
      "grad_norm": 1.6955474615097046,
      "learning_rate": 1.5523248199083173e-05,
      "loss": 1.156,
      "step": 1710
    },
    {
      "epoch": 0.6758349705304518,
      "grad_norm": 1.617496132850647,
      "learning_rate": 1.5497053045186642e-05,
      "loss": 1.0981,
      "step": 1720
    },
    {
      "epoch": 0.6797642436149313,
      "grad_norm": 1.562425971031189,
      "learning_rate": 1.547085789129011e-05,
      "loss": 1.1761,
      "step": 1730
    },
    {
      "epoch": 0.6836935166994106,
      "grad_norm": 1.569726586341858,
      "learning_rate": 1.5444662737393583e-05,
      "loss": 1.1955,
      "step": 1740
    },
    {
      "epoch": 0.68762278978389,
      "grad_norm": 1.4658849239349365,
      "learning_rate": 1.5418467583497055e-05,
      "loss": 1.1391,
      "step": 1750
    },
    {
      "epoch": 0.6915520628683693,
      "grad_norm": 1.8080109357833862,
      "learning_rate": 1.5392272429600527e-05,
      "loss": 1.1054,
      "step": 1760
    },
    {
      "epoch": 0.6954813359528488,
      "grad_norm": 1.9116578102111816,
      "learning_rate": 1.5366077275703995e-05,
      "loss": 1.173,
      "step": 1770
    },
    {
      "epoch": 0.6994106090373281,
      "grad_norm": 1.7585715055465698,
      "learning_rate": 1.5339882121807467e-05,
      "loss": 1.1897,
      "step": 1780
    },
    {
      "epoch": 0.7033398821218074,
      "grad_norm": 1.5836536884307861,
      "learning_rate": 1.5313686967910936e-05,
      "loss": 1.137,
      "step": 1790
    },
    {
      "epoch": 0.7072691552062869,
      "grad_norm": 1.3936691284179688,
      "learning_rate": 1.5287491814014408e-05,
      "loss": 1.1815,
      "step": 1800
    },
    {
      "epoch": 0.7111984282907662,
      "grad_norm": 1.676542043685913,
      "learning_rate": 1.526129666011788e-05,
      "loss": 1.1242,
      "step": 1810
    },
    {
      "epoch": 0.7151277013752456,
      "grad_norm": 1.8706055879592896,
      "learning_rate": 1.523510150622135e-05,
      "loss": 1.1625,
      "step": 1820
    },
    {
      "epoch": 0.7190569744597249,
      "grad_norm": 1.8732541799545288,
      "learning_rate": 1.5208906352324822e-05,
      "loss": 1.1583,
      "step": 1830
    },
    {
      "epoch": 0.7229862475442044,
      "grad_norm": 1.7812002897262573,
      "learning_rate": 1.5182711198428293e-05,
      "loss": 1.1298,
      "step": 1840
    },
    {
      "epoch": 0.7269155206286837,
      "grad_norm": 1.6778945922851562,
      "learning_rate": 1.5156516044531765e-05,
      "loss": 1.1314,
      "step": 1850
    },
    {
      "epoch": 0.730844793713163,
      "grad_norm": 2.1457419395446777,
      "learning_rate": 1.5130320890635233e-05,
      "loss": 1.1821,
      "step": 1860
    },
    {
      "epoch": 0.7347740667976425,
      "grad_norm": 1.5373802185058594,
      "learning_rate": 1.5104125736738703e-05,
      "loss": 1.2088,
      "step": 1870
    },
    {
      "epoch": 0.7387033398821218,
      "grad_norm": 1.603118658065796,
      "learning_rate": 1.5077930582842175e-05,
      "loss": 1.1706,
      "step": 1880
    },
    {
      "epoch": 0.7426326129666012,
      "grad_norm": 1.91189706325531,
      "learning_rate": 1.5051735428945646e-05,
      "loss": 1.141,
      "step": 1890
    },
    {
      "epoch": 0.7465618860510805,
      "grad_norm": 1.5330692529678345,
      "learning_rate": 1.5025540275049118e-05,
      "loss": 1.1812,
      "step": 1900
    },
    {
      "epoch": 0.75049115913556,
      "grad_norm": 1.564414143562317,
      "learning_rate": 1.4999345121152588e-05,
      "loss": 1.128,
      "step": 1910
    },
    {
      "epoch": 0.7544204322200393,
      "grad_norm": 3.4704973697662354,
      "learning_rate": 1.497314996725606e-05,
      "loss": 1.1148,
      "step": 1920
    },
    {
      "epoch": 0.7583497053045186,
      "grad_norm": 1.7507641315460205,
      "learning_rate": 1.4946954813359529e-05,
      "loss": 1.1319,
      "step": 1930
    },
    {
      "epoch": 0.762278978388998,
      "grad_norm": 1.9349303245544434,
      "learning_rate": 1.4920759659463e-05,
      "loss": 1.2059,
      "step": 1940
    },
    {
      "epoch": 0.7662082514734774,
      "grad_norm": 1.574121356010437,
      "learning_rate": 1.4894564505566471e-05,
      "loss": 1.1329,
      "step": 1950
    },
    {
      "epoch": 0.7701375245579568,
      "grad_norm": 1.6555116176605225,
      "learning_rate": 1.4868369351669943e-05,
      "loss": 1.1958,
      "step": 1960
    },
    {
      "epoch": 0.7740667976424361,
      "grad_norm": 1.7289568185806274,
      "learning_rate": 1.4842174197773413e-05,
      "loss": 1.1255,
      "step": 1970
    },
    {
      "epoch": 0.7779960707269156,
      "grad_norm": 1.6021485328674316,
      "learning_rate": 1.4815979043876885e-05,
      "loss": 1.1159,
      "step": 1980
    },
    {
      "epoch": 0.7819253438113949,
      "grad_norm": 1.7704764604568481,
      "learning_rate": 1.4789783889980356e-05,
      "loss": 1.0921,
      "step": 1990
    },
    {
      "epoch": 0.7858546168958742,
      "grad_norm": 1.6832592487335205,
      "learning_rate": 1.4763588736083824e-05,
      "loss": 1.1433,
      "step": 2000
    },
    {
      "epoch": 0.7897838899803536,
      "grad_norm": 1.7384663820266724,
      "learning_rate": 1.4737393582187296e-05,
      "loss": 1.1211,
      "step": 2010
    },
    {
      "epoch": 0.793713163064833,
      "grad_norm": 1.8997180461883545,
      "learning_rate": 1.4711198428290767e-05,
      "loss": 1.1697,
      "step": 2020
    },
    {
      "epoch": 0.7976424361493124,
      "grad_norm": 1.7262159585952759,
      "learning_rate": 1.4685003274394239e-05,
      "loss": 1.1807,
      "step": 2030
    },
    {
      "epoch": 0.8015717092337917,
      "grad_norm": 1.8351216316223145,
      "learning_rate": 1.4658808120497709e-05,
      "loss": 1.1512,
      "step": 2040
    },
    {
      "epoch": 0.8055009823182712,
      "grad_norm": 1.788955569267273,
      "learning_rate": 1.4632612966601181e-05,
      "loss": 1.1818,
      "step": 2050
    },
    {
      "epoch": 0.8094302554027505,
      "grad_norm": 1.9728387594223022,
      "learning_rate": 1.4606417812704651e-05,
      "loss": 1.2176,
      "step": 2060
    },
    {
      "epoch": 0.8133595284872298,
      "grad_norm": 2.451129913330078,
      "learning_rate": 1.4580222658808122e-05,
      "loss": 1.0917,
      "step": 2070
    },
    {
      "epoch": 0.8172888015717092,
      "grad_norm": 2.0084712505340576,
      "learning_rate": 1.4554027504911592e-05,
      "loss": 1.1248,
      "step": 2080
    },
    {
      "epoch": 0.8212180746561886,
      "grad_norm": 1.9747775793075562,
      "learning_rate": 1.4527832351015064e-05,
      "loss": 1.1095,
      "step": 2090
    },
    {
      "epoch": 0.825147347740668,
      "grad_norm": 1.9331626892089844,
      "learning_rate": 1.4501637197118534e-05,
      "loss": 1.1939,
      "step": 2100
    },
    {
      "epoch": 0.8290766208251473,
      "grad_norm": 1.7860097885131836,
      "learning_rate": 1.4475442043222006e-05,
      "loss": 1.1757,
      "step": 2110
    },
    {
      "epoch": 0.8330058939096268,
      "grad_norm": 1.6361809968948364,
      "learning_rate": 1.4449246889325476e-05,
      "loss": 1.1668,
      "step": 2120
    },
    {
      "epoch": 0.8369351669941061,
      "grad_norm": 1.7419376373291016,
      "learning_rate": 1.4423051735428948e-05,
      "loss": 1.1216,
      "step": 2130
    },
    {
      "epoch": 0.8408644400785854,
      "grad_norm": 1.6152530908584595,
      "learning_rate": 1.4396856581532417e-05,
      "loss": 1.1617,
      "step": 2140
    },
    {
      "epoch": 0.8447937131630648,
      "grad_norm": 1.7061278820037842,
      "learning_rate": 1.4370661427635887e-05,
      "loss": 1.1415,
      "step": 2150
    },
    {
      "epoch": 0.8487229862475442,
      "grad_norm": 1.6819978952407837,
      "learning_rate": 1.434446627373936e-05,
      "loss": 1.1819,
      "step": 2160
    },
    {
      "epoch": 0.8526522593320236,
      "grad_norm": 2.260586977005005,
      "learning_rate": 1.431827111984283e-05,
      "loss": 1.1365,
      "step": 2170
    },
    {
      "epoch": 0.8565815324165029,
      "grad_norm": 1.8148540258407593,
      "learning_rate": 1.4292075965946302e-05,
      "loss": 1.1154,
      "step": 2180
    },
    {
      "epoch": 0.8605108055009824,
      "grad_norm": 1.626731038093567,
      "learning_rate": 1.4265880812049772e-05,
      "loss": 1.1243,
      "step": 2190
    },
    {
      "epoch": 0.8644400785854617,
      "grad_norm": 1.870645523071289,
      "learning_rate": 1.4239685658153244e-05,
      "loss": 1.1548,
      "step": 2200
    },
    {
      "epoch": 0.8683693516699411,
      "grad_norm": 1.636791467666626,
      "learning_rate": 1.4213490504256713e-05,
      "loss": 1.1514,
      "step": 2210
    },
    {
      "epoch": 0.8722986247544204,
      "grad_norm": 1.7674013376235962,
      "learning_rate": 1.4187295350360185e-05,
      "loss": 1.1396,
      "step": 2220
    },
    {
      "epoch": 0.8762278978388998,
      "grad_norm": 1.7319912910461426,
      "learning_rate": 1.4161100196463655e-05,
      "loss": 1.1768,
      "step": 2230
    },
    {
      "epoch": 0.8801571709233792,
      "grad_norm": 1.9988093376159668,
      "learning_rate": 1.4134905042567127e-05,
      "loss": 1.1024,
      "step": 2240
    },
    {
      "epoch": 0.8840864440078585,
      "grad_norm": 1.8315718173980713,
      "learning_rate": 1.4108709888670597e-05,
      "loss": 1.1338,
      "step": 2250
    },
    {
      "epoch": 0.888015717092338,
      "grad_norm": 1.8207008838653564,
      "learning_rate": 1.408251473477407e-05,
      "loss": 1.157,
      "step": 2260
    },
    {
      "epoch": 0.8919449901768173,
      "grad_norm": 1.9370981454849243,
      "learning_rate": 1.405631958087754e-05,
      "loss": 1.1226,
      "step": 2270
    },
    {
      "epoch": 0.8958742632612967,
      "grad_norm": 1.7501158714294434,
      "learning_rate": 1.4030124426981008e-05,
      "loss": 1.0811,
      "step": 2280
    },
    {
      "epoch": 0.899803536345776,
      "grad_norm": 2.1324520111083984,
      "learning_rate": 1.400392927308448e-05,
      "loss": 1.2349,
      "step": 2290
    },
    {
      "epoch": 0.9037328094302554,
      "grad_norm": 1.6558057069778442,
      "learning_rate": 1.397773411918795e-05,
      "loss": 1.1376,
      "step": 2300
    },
    {
      "epoch": 0.9076620825147348,
      "grad_norm": 1.8229252099990845,
      "learning_rate": 1.3951538965291422e-05,
      "loss": 1.1749,
      "step": 2310
    },
    {
      "epoch": 0.9115913555992141,
      "grad_norm": 1.696044683456421,
      "learning_rate": 1.3925343811394893e-05,
      "loss": 1.166,
      "step": 2320
    },
    {
      "epoch": 0.9155206286836935,
      "grad_norm": 1.8439825773239136,
      "learning_rate": 1.3899148657498365e-05,
      "loss": 1.1097,
      "step": 2330
    },
    {
      "epoch": 0.9194499017681729,
      "grad_norm": 2.082125186920166,
      "learning_rate": 1.3872953503601835e-05,
      "loss": 1.118,
      "step": 2340
    },
    {
      "epoch": 0.9233791748526523,
      "grad_norm": 1.9174333810806274,
      "learning_rate": 1.3846758349705305e-05,
      "loss": 1.0901,
      "step": 2350
    },
    {
      "epoch": 0.9273084479371316,
      "grad_norm": 1.8872805833816528,
      "learning_rate": 1.3820563195808776e-05,
      "loss": 1.1813,
      "step": 2360
    },
    {
      "epoch": 0.931237721021611,
      "grad_norm": 1.6934748888015747,
      "learning_rate": 1.3794368041912248e-05,
      "loss": 1.1,
      "step": 2370
    },
    {
      "epoch": 0.9351669941060904,
      "grad_norm": 1.7476340532302856,
      "learning_rate": 1.3768172888015718e-05,
      "loss": 1.1412,
      "step": 2380
    },
    {
      "epoch": 0.9390962671905697,
      "grad_norm": 1.9639722108840942,
      "learning_rate": 1.374197773411919e-05,
      "loss": 1.1259,
      "step": 2390
    },
    {
      "epoch": 0.9430255402750491,
      "grad_norm": 2.2834033966064453,
      "learning_rate": 1.371578258022266e-05,
      "loss": 1.1113,
      "step": 2400
    },
    {
      "epoch": 0.9469548133595285,
      "grad_norm": 1.9406695365905762,
      "learning_rate": 1.3689587426326129e-05,
      "loss": 1.1367,
      "step": 2410
    },
    {
      "epoch": 0.9508840864440079,
      "grad_norm": 1.7408020496368408,
      "learning_rate": 1.3663392272429601e-05,
      "loss": 1.1066,
      "step": 2420
    },
    {
      "epoch": 0.9548133595284872,
      "grad_norm": 1.5422258377075195,
      "learning_rate": 1.3637197118533071e-05,
      "loss": 1.174,
      "step": 2430
    },
    {
      "epoch": 0.9587426326129665,
      "grad_norm": 1.9747514724731445,
      "learning_rate": 1.3611001964636543e-05,
      "loss": 1.1262,
      "step": 2440
    },
    {
      "epoch": 0.962671905697446,
      "grad_norm": 1.850495457649231,
      "learning_rate": 1.3584806810740014e-05,
      "loss": 1.1949,
      "step": 2450
    },
    {
      "epoch": 0.9666011787819253,
      "grad_norm": 1.468515396118164,
      "learning_rate": 1.3558611656843486e-05,
      "loss": 1.1136,
      "step": 2460
    },
    {
      "epoch": 0.9705304518664047,
      "grad_norm": 1.7953037023544312,
      "learning_rate": 1.3532416502946958e-05,
      "loss": 1.0594,
      "step": 2470
    },
    {
      "epoch": 0.9744597249508841,
      "grad_norm": 2.3204922676086426,
      "learning_rate": 1.3506221349050426e-05,
      "loss": 1.1784,
      "step": 2480
    },
    {
      "epoch": 0.9783889980353635,
      "grad_norm": 1.9514553546905518,
      "learning_rate": 1.3480026195153897e-05,
      "loss": 1.101,
      "step": 2490
    },
    {
      "epoch": 0.9823182711198428,
      "grad_norm": 1.8242487907409668,
      "learning_rate": 1.3453831041257369e-05,
      "loss": 1.1141,
      "step": 2500
    },
    {
      "epoch": 0.9862475442043221,
      "grad_norm": 2.010221242904663,
      "learning_rate": 1.3427635887360839e-05,
      "loss": 1.1572,
      "step": 2510
    },
    {
      "epoch": 0.9901768172888016,
      "grad_norm": 1.8046318292617798,
      "learning_rate": 1.340144073346431e-05,
      "loss": 1.1449,
      "step": 2520
    },
    {
      "epoch": 0.9941060903732809,
      "grad_norm": 1.8176076412200928,
      "learning_rate": 1.3375245579567781e-05,
      "loss": 1.1861,
      "step": 2530
    },
    {
      "epoch": 0.9980353634577603,
      "grad_norm": 1.6897011995315552,
      "learning_rate": 1.3349050425671253e-05,
      "loss": 1.1298,
      "step": 2540
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.1388226747512817,
      "eval_runtime": 50.7362,
      "eval_samples_per_second": 25.091,
      "eval_steps_per_second": 6.287,
      "step": 2545
    },
    {
      "epoch": 1.0019646365422397,
      "grad_norm": 1.7396652698516846,
      "learning_rate": 1.3322855271774722e-05,
      "loss": 1.1086,
      "step": 2550
    },
    {
      "epoch": 1.005893909626719,
      "grad_norm": 1.8250629901885986,
      "learning_rate": 1.3296660117878192e-05,
      "loss": 1.1321,
      "step": 2560
    },
    {
      "epoch": 1.0098231827111985,
      "grad_norm": 1.889578938484192,
      "learning_rate": 1.3270464963981664e-05,
      "loss": 1.1523,
      "step": 2570
    },
    {
      "epoch": 1.0137524557956779,
      "grad_norm": 2.0246734619140625,
      "learning_rate": 1.3244269810085136e-05,
      "loss": 1.1031,
      "step": 2580
    },
    {
      "epoch": 1.0176817288801572,
      "grad_norm": 1.7299182415008545,
      "learning_rate": 1.3218074656188606e-05,
      "loss": 1.147,
      "step": 2590
    },
    {
      "epoch": 1.0216110019646365,
      "grad_norm": 1.747438907623291,
      "learning_rate": 1.3191879502292078e-05,
      "loss": 1.1229,
      "step": 2600
    },
    {
      "epoch": 1.0255402750491158,
      "grad_norm": 1.7392387390136719,
      "learning_rate": 1.3165684348395549e-05,
      "loss": 1.1681,
      "step": 2610
    },
    {
      "epoch": 1.0294695481335954,
      "grad_norm": 1.5176295042037964,
      "learning_rate": 1.3139489194499017e-05,
      "loss": 1.1198,
      "step": 2620
    },
    {
      "epoch": 1.0333988212180747,
      "grad_norm": 1.68696928024292,
      "learning_rate": 1.311329404060249e-05,
      "loss": 1.1417,
      "step": 2630
    },
    {
      "epoch": 1.037328094302554,
      "grad_norm": 1.8024320602416992,
      "learning_rate": 1.308709888670596e-05,
      "loss": 1.1571,
      "step": 2640
    },
    {
      "epoch": 1.0412573673870333,
      "grad_norm": 2.0006418228149414,
      "learning_rate": 1.3060903732809432e-05,
      "loss": 1.1314,
      "step": 2650
    },
    {
      "epoch": 1.0451866404715129,
      "grad_norm": 1.9615355730056763,
      "learning_rate": 1.3034708578912902e-05,
      "loss": 1.1021,
      "step": 2660
    },
    {
      "epoch": 1.0491159135559922,
      "grad_norm": 2.0046615600585938,
      "learning_rate": 1.3008513425016374e-05,
      "loss": 1.1977,
      "step": 2670
    },
    {
      "epoch": 1.0530451866404715,
      "grad_norm": 1.9078021049499512,
      "learning_rate": 1.2982318271119844e-05,
      "loss": 1.1663,
      "step": 2680
    },
    {
      "epoch": 1.0569744597249509,
      "grad_norm": 1.685477614402771,
      "learning_rate": 1.2956123117223315e-05,
      "loss": 1.1622,
      "step": 2690
    },
    {
      "epoch": 1.0609037328094302,
      "grad_norm": 1.741052269935608,
      "learning_rate": 1.2929927963326785e-05,
      "loss": 1.1392,
      "step": 2700
    },
    {
      "epoch": 1.0648330058939097,
      "grad_norm": 1.8617042303085327,
      "learning_rate": 1.2903732809430257e-05,
      "loss": 1.0857,
      "step": 2710
    },
    {
      "epoch": 1.068762278978389,
      "grad_norm": 1.8254046440124512,
      "learning_rate": 1.2877537655533727e-05,
      "loss": 1.0901,
      "step": 2720
    },
    {
      "epoch": 1.0726915520628684,
      "grad_norm": 2.0612452030181885,
      "learning_rate": 1.28513425016372e-05,
      "loss": 1.1434,
      "step": 2730
    },
    {
      "epoch": 1.0766208251473477,
      "grad_norm": 1.7015751600265503,
      "learning_rate": 1.282514734774067e-05,
      "loss": 1.1275,
      "step": 2740
    },
    {
      "epoch": 1.080550098231827,
      "grad_norm": 1.6800471544265747,
      "learning_rate": 1.2798952193844141e-05,
      "loss": 1.136,
      "step": 2750
    },
    {
      "epoch": 1.0844793713163066,
      "grad_norm": 1.8444360494613647,
      "learning_rate": 1.277275703994761e-05,
      "loss": 1.1004,
      "step": 2760
    },
    {
      "epoch": 1.0884086444007859,
      "grad_norm": 2.022963762283325,
      "learning_rate": 1.274656188605108e-05,
      "loss": 1.1331,
      "step": 2770
    },
    {
      "epoch": 1.0923379174852652,
      "grad_norm": 1.8501774072647095,
      "learning_rate": 1.2720366732154552e-05,
      "loss": 1.1344,
      "step": 2780
    },
    {
      "epoch": 1.0962671905697445,
      "grad_norm": 1.781044602394104,
      "learning_rate": 1.2694171578258023e-05,
      "loss": 1.119,
      "step": 2790
    },
    {
      "epoch": 1.1001964636542239,
      "grad_norm": 2.0395123958587646,
      "learning_rate": 1.2667976424361495e-05,
      "loss": 1.1395,
      "step": 2800
    },
    {
      "epoch": 1.1041257367387034,
      "grad_norm": 1.8161654472351074,
      "learning_rate": 1.2641781270464965e-05,
      "loss": 1.1143,
      "step": 2810
    },
    {
      "epoch": 1.1080550098231827,
      "grad_norm": 2.117494583129883,
      "learning_rate": 1.2615586116568437e-05,
      "loss": 1.0913,
      "step": 2820
    },
    {
      "epoch": 1.111984282907662,
      "grad_norm": 1.6742043495178223,
      "learning_rate": 1.2589390962671906e-05,
      "loss": 1.116,
      "step": 2830
    },
    {
      "epoch": 1.1159135559921414,
      "grad_norm": 1.841413140296936,
      "learning_rate": 1.2563195808775378e-05,
      "loss": 1.1112,
      "step": 2840
    },
    {
      "epoch": 1.119842829076621,
      "grad_norm": 1.7501237392425537,
      "learning_rate": 1.2537000654878848e-05,
      "loss": 1.124,
      "step": 2850
    },
    {
      "epoch": 1.1237721021611002,
      "grad_norm": 2.0523030757904053,
      "learning_rate": 1.251080550098232e-05,
      "loss": 1.1132,
      "step": 2860
    },
    {
      "epoch": 1.1277013752455796,
      "grad_norm": 1.956774115562439,
      "learning_rate": 1.248461034708579e-05,
      "loss": 1.1266,
      "step": 2870
    },
    {
      "epoch": 1.1316306483300589,
      "grad_norm": 2.0799460411071777,
      "learning_rate": 1.2458415193189262e-05,
      "loss": 1.1644,
      "step": 2880
    },
    {
      "epoch": 1.1355599214145382,
      "grad_norm": 1.9894028902053833,
      "learning_rate": 1.2432220039292733e-05,
      "loss": 1.0606,
      "step": 2890
    },
    {
      "epoch": 1.1394891944990178,
      "grad_norm": 1.7257120609283447,
      "learning_rate": 1.2406024885396201e-05,
      "loss": 1.1601,
      "step": 2900
    },
    {
      "epoch": 1.143418467583497,
      "grad_norm": 2.0757293701171875,
      "learning_rate": 1.2379829731499673e-05,
      "loss": 1.1168,
      "step": 2910
    },
    {
      "epoch": 1.1473477406679764,
      "grad_norm": 1.6013259887695312,
      "learning_rate": 1.2353634577603144e-05,
      "loss": 1.1616,
      "step": 2920
    },
    {
      "epoch": 1.1512770137524557,
      "grad_norm": 1.630890965461731,
      "learning_rate": 1.2327439423706616e-05,
      "loss": 1.0609,
      "step": 2930
    },
    {
      "epoch": 1.1552062868369353,
      "grad_norm": 1.7253329753875732,
      "learning_rate": 1.2301244269810086e-05,
      "loss": 1.1422,
      "step": 2940
    },
    {
      "epoch": 1.1591355599214146,
      "grad_norm": 1.7643059492111206,
      "learning_rate": 1.2275049115913558e-05,
      "loss": 1.1263,
      "step": 2950
    },
    {
      "epoch": 1.163064833005894,
      "grad_norm": 1.7061703205108643,
      "learning_rate": 1.2248853962017028e-05,
      "loss": 1.0832,
      "step": 2960
    },
    {
      "epoch": 1.1669941060903732,
      "grad_norm": 1.7645041942596436,
      "learning_rate": 1.2222658808120498e-05,
      "loss": 1.0749,
      "step": 2970
    },
    {
      "epoch": 1.1709233791748526,
      "grad_norm": 1.977035641670227,
      "learning_rate": 1.2196463654223969e-05,
      "loss": 1.1578,
      "step": 2980
    },
    {
      "epoch": 1.174852652259332,
      "grad_norm": 1.70768404006958,
      "learning_rate": 1.217026850032744e-05,
      "loss": 1.1288,
      "step": 2990
    },
    {
      "epoch": 1.1787819253438114,
      "grad_norm": 1.8395280838012695,
      "learning_rate": 1.2144073346430911e-05,
      "loss": 1.1608,
      "step": 3000
    },
    {
      "epoch": 1.1827111984282908,
      "grad_norm": 1.7936259508132935,
      "learning_rate": 1.2117878192534383e-05,
      "loss": 1.1394,
      "step": 3010
    },
    {
      "epoch": 1.18664047151277,
      "grad_norm": 1.661059856414795,
      "learning_rate": 1.2091683038637853e-05,
      "loss": 1.106,
      "step": 3020
    },
    {
      "epoch": 1.1905697445972496,
      "grad_norm": 1.8392711877822876,
      "learning_rate": 1.2065487884741325e-05,
      "loss": 1.1676,
      "step": 3030
    },
    {
      "epoch": 1.194499017681729,
      "grad_norm": 1.9899609088897705,
      "learning_rate": 1.2039292730844794e-05,
      "loss": 1.0904,
      "step": 3040
    },
    {
      "epoch": 1.1984282907662083,
      "grad_norm": 2.318288803100586,
      "learning_rate": 1.2013097576948264e-05,
      "loss": 1.0971,
      "step": 3050
    },
    {
      "epoch": 1.2023575638506876,
      "grad_norm": 2.18414044380188,
      "learning_rate": 1.1986902423051736e-05,
      "loss": 1.1078,
      "step": 3060
    },
    {
      "epoch": 1.206286836935167,
      "grad_norm": 2.1163840293884277,
      "learning_rate": 1.1960707269155207e-05,
      "loss": 1.1267,
      "step": 3070
    },
    {
      "epoch": 1.2102161100196462,
      "grad_norm": 1.893000602722168,
      "learning_rate": 1.1934512115258679e-05,
      "loss": 1.1108,
      "step": 3080
    },
    {
      "epoch": 1.2141453831041258,
      "grad_norm": 1.828933835029602,
      "learning_rate": 1.1908316961362149e-05,
      "loss": 1.1122,
      "step": 3090
    },
    {
      "epoch": 1.218074656188605,
      "grad_norm": 1.9451754093170166,
      "learning_rate": 1.1882121807465621e-05,
      "loss": 1.1215,
      "step": 3100
    },
    {
      "epoch": 1.2220039292730844,
      "grad_norm": 1.8122104406356812,
      "learning_rate": 1.185592665356909e-05,
      "loss": 1.0878,
      "step": 3110
    },
    {
      "epoch": 1.2259332023575638,
      "grad_norm": 1.938112735748291,
      "learning_rate": 1.1829731499672562e-05,
      "loss": 1.0653,
      "step": 3120
    },
    {
      "epoch": 1.2298624754420433,
      "grad_norm": 2.408111095428467,
      "learning_rate": 1.1803536345776032e-05,
      "loss": 1.124,
      "step": 3130
    },
    {
      "epoch": 1.2337917485265226,
      "grad_norm": 2.034074068069458,
      "learning_rate": 1.1777341191879504e-05,
      "loss": 1.1108,
      "step": 3140
    },
    {
      "epoch": 1.237721021611002,
      "grad_norm": 2.7406864166259766,
      "learning_rate": 1.1751146037982974e-05,
      "loss": 1.1284,
      "step": 3150
    },
    {
      "epoch": 1.2416502946954813,
      "grad_norm": 1.9139339923858643,
      "learning_rate": 1.1724950884086446e-05,
      "loss": 1.1442,
      "step": 3160
    },
    {
      "epoch": 1.2455795677799606,
      "grad_norm": 1.937361717224121,
      "learning_rate": 1.1698755730189916e-05,
      "loss": 1.116,
      "step": 3170
    },
    {
      "epoch": 1.2495088408644401,
      "grad_norm": 2.024574041366577,
      "learning_rate": 1.1672560576293385e-05,
      "loss": 1.147,
      "step": 3180
    },
    {
      "epoch": 1.2534381139489195,
      "grad_norm": 1.7446283102035522,
      "learning_rate": 1.1646365422396857e-05,
      "loss": 1.1805,
      "step": 3190
    },
    {
      "epoch": 1.2573673870333988,
      "grad_norm": 1.896459937095642,
      "learning_rate": 1.1620170268500327e-05,
      "loss": 1.0829,
      "step": 3200
    },
    {
      "epoch": 1.261296660117878,
      "grad_norm": 2.0135257244110107,
      "learning_rate": 1.15939751146038e-05,
      "loss": 1.0727,
      "step": 3210
    },
    {
      "epoch": 1.2652259332023577,
      "grad_norm": 1.8619105815887451,
      "learning_rate": 1.156777996070727e-05,
      "loss": 1.1282,
      "step": 3220
    },
    {
      "epoch": 1.269155206286837,
      "grad_norm": 1.7236542701721191,
      "learning_rate": 1.1541584806810742e-05,
      "loss": 1.1318,
      "step": 3230
    },
    {
      "epoch": 1.2730844793713163,
      "grad_norm": 1.660662293434143,
      "learning_rate": 1.1515389652914212e-05,
      "loss": 1.1252,
      "step": 3240
    },
    {
      "epoch": 1.2770137524557956,
      "grad_norm": 1.9800608158111572,
      "learning_rate": 1.1489194499017682e-05,
      "loss": 1.1564,
      "step": 3250
    },
    {
      "epoch": 1.280943025540275,
      "grad_norm": 1.9082359075546265,
      "learning_rate": 1.1462999345121153e-05,
      "loss": 1.0831,
      "step": 3260
    },
    {
      "epoch": 1.2848722986247545,
      "grad_norm": 2.1926960945129395,
      "learning_rate": 1.1436804191224625e-05,
      "loss": 1.1897,
      "step": 3270
    },
    {
      "epoch": 1.2888015717092338,
      "grad_norm": 2.07265043258667,
      "learning_rate": 1.1410609037328095e-05,
      "loss": 1.1275,
      "step": 3280
    },
    {
      "epoch": 1.2927308447937131,
      "grad_norm": 1.9597970247268677,
      "learning_rate": 1.1384413883431567e-05,
      "loss": 1.1331,
      "step": 3290
    },
    {
      "epoch": 1.2966601178781925,
      "grad_norm": 1.7419687509536743,
      "learning_rate": 1.1358218729535037e-05,
      "loss": 1.1538,
      "step": 3300
    },
    {
      "epoch": 1.300589390962672,
      "grad_norm": 2.256744384765625,
      "learning_rate": 1.133202357563851e-05,
      "loss": 1.1979,
      "step": 3310
    },
    {
      "epoch": 1.3045186640471513,
      "grad_norm": 1.8945448398590088,
      "learning_rate": 1.1305828421741978e-05,
      "loss": 1.0886,
      "step": 3320
    },
    {
      "epoch": 1.3084479371316307,
      "grad_norm": 1.6824405193328857,
      "learning_rate": 1.1279633267845448e-05,
      "loss": 1.1466,
      "step": 3330
    },
    {
      "epoch": 1.31237721021611,
      "grad_norm": 1.6991667747497559,
      "learning_rate": 1.125343811394892e-05,
      "loss": 1.1263,
      "step": 3340
    },
    {
      "epoch": 1.3163064833005893,
      "grad_norm": 2.2507247924804688,
      "learning_rate": 1.122724296005239e-05,
      "loss": 1.1068,
      "step": 3350
    },
    {
      "epoch": 1.3202357563850686,
      "grad_norm": 1.8000603914260864,
      "learning_rate": 1.1201047806155863e-05,
      "loss": 1.0951,
      "step": 3360
    },
    {
      "epoch": 1.3241650294695482,
      "grad_norm": 1.977052927017212,
      "learning_rate": 1.1174852652259335e-05,
      "loss": 1.0836,
      "step": 3370
    },
    {
      "epoch": 1.3280943025540275,
      "grad_norm": 1.7385143041610718,
      "learning_rate": 1.1148657498362805e-05,
      "loss": 1.1098,
      "step": 3380
    },
    {
      "epoch": 1.3320235756385068,
      "grad_norm": 1.8601430654525757,
      "learning_rate": 1.1122462344466273e-05,
      "loss": 1.154,
      "step": 3390
    },
    {
      "epoch": 1.3359528487229864,
      "grad_norm": 1.6374222040176392,
      "learning_rate": 1.1096267190569745e-05,
      "loss": 1.1188,
      "step": 3400
    },
    {
      "epoch": 1.3398821218074657,
      "grad_norm": 1.8854148387908936,
      "learning_rate": 1.1070072036673216e-05,
      "loss": 1.1103,
      "step": 3410
    },
    {
      "epoch": 1.343811394891945,
      "grad_norm": 2.0367987155914307,
      "learning_rate": 1.1043876882776688e-05,
      "loss": 1.1784,
      "step": 3420
    },
    {
      "epoch": 1.3477406679764243,
      "grad_norm": 2.328357458114624,
      "learning_rate": 1.1017681728880158e-05,
      "loss": 1.1739,
      "step": 3430
    },
    {
      "epoch": 1.3516699410609037,
      "grad_norm": 1.9369748830795288,
      "learning_rate": 1.099148657498363e-05,
      "loss": 1.1854,
      "step": 3440
    },
    {
      "epoch": 1.355599214145383,
      "grad_norm": 2.1982216835021973,
      "learning_rate": 1.09652914210871e-05,
      "loss": 1.1522,
      "step": 3450
    },
    {
      "epoch": 1.3595284872298625,
      "grad_norm": 1.6964601278305054,
      "learning_rate": 1.0939096267190569e-05,
      "loss": 1.1119,
      "step": 3460
    },
    {
      "epoch": 1.3634577603143418,
      "grad_norm": 1.6950836181640625,
      "learning_rate": 1.0912901113294041e-05,
      "loss": 1.1038,
      "step": 3470
    },
    {
      "epoch": 1.3673870333988212,
      "grad_norm": 1.8187508583068848,
      "learning_rate": 1.0886705959397513e-05,
      "loss": 1.1745,
      "step": 3480
    },
    {
      "epoch": 1.3713163064833007,
      "grad_norm": 1.7548186779022217,
      "learning_rate": 1.0860510805500983e-05,
      "loss": 1.1546,
      "step": 3490
    },
    {
      "epoch": 1.37524557956778,
      "grad_norm": 2.2558863162994385,
      "learning_rate": 1.0834315651604455e-05,
      "loss": 1.0973,
      "step": 3500
    },
    {
      "epoch": 1.3791748526522594,
      "grad_norm": 2.2811524868011475,
      "learning_rate": 1.0808120497707926e-05,
      "loss": 1.1081,
      "step": 3510
    },
    {
      "epoch": 1.3831041257367387,
      "grad_norm": 1.8494937419891357,
      "learning_rate": 1.0781925343811398e-05,
      "loss": 1.1013,
      "step": 3520
    },
    {
      "epoch": 1.387033398821218,
      "grad_norm": 2.0987026691436768,
      "learning_rate": 1.0755730189914866e-05,
      "loss": 1.1599,
      "step": 3530
    },
    {
      "epoch": 1.3909626719056973,
      "grad_norm": 1.8814998865127563,
      "learning_rate": 1.0729535036018337e-05,
      "loss": 1.1386,
      "step": 3540
    },
    {
      "epoch": 1.3948919449901769,
      "grad_norm": 2.0238430500030518,
      "learning_rate": 1.0703339882121809e-05,
      "loss": 1.0855,
      "step": 3550
    },
    {
      "epoch": 1.3988212180746562,
      "grad_norm": 2.027888536453247,
      "learning_rate": 1.0677144728225279e-05,
      "loss": 1.1044,
      "step": 3560
    },
    {
      "epoch": 1.4027504911591355,
      "grad_norm": 2.0724966526031494,
      "learning_rate": 1.0650949574328751e-05,
      "loss": 1.1005,
      "step": 3570
    },
    {
      "epoch": 1.4066797642436148,
      "grad_norm": 2.3610856533050537,
      "learning_rate": 1.0624754420432221e-05,
      "loss": 1.1099,
      "step": 3580
    },
    {
      "epoch": 1.4106090373280944,
      "grad_norm": 1.8807305097579956,
      "learning_rate": 1.0598559266535691e-05,
      "loss": 1.1545,
      "step": 3590
    },
    {
      "epoch": 1.4145383104125737,
      "grad_norm": 1.6609718799591064,
      "learning_rate": 1.0572364112639162e-05,
      "loss": 1.156,
      "step": 3600
    },
    {
      "epoch": 1.418467583497053,
      "grad_norm": 2.10435152053833,
      "learning_rate": 1.0546168958742634e-05,
      "loss": 1.154,
      "step": 3610
    },
    {
      "epoch": 1.4223968565815324,
      "grad_norm": 1.8111841678619385,
      "learning_rate": 1.0519973804846104e-05,
      "loss": 1.1481,
      "step": 3620
    },
    {
      "epoch": 1.4263261296660117,
      "grad_norm": 2.1389620304107666,
      "learning_rate": 1.0493778650949576e-05,
      "loss": 1.14,
      "step": 3630
    },
    {
      "epoch": 1.4302554027504912,
      "grad_norm": 1.8192030191421509,
      "learning_rate": 1.0467583497053046e-05,
      "loss": 1.1285,
      "step": 3640
    },
    {
      "epoch": 1.4341846758349706,
      "grad_norm": 1.8720431327819824,
      "learning_rate": 1.0441388343156518e-05,
      "loss": 1.108,
      "step": 3650
    },
    {
      "epoch": 1.4381139489194499,
      "grad_norm": 1.9723231792449951,
      "learning_rate": 1.0415193189259987e-05,
      "loss": 1.0393,
      "step": 3660
    },
    {
      "epoch": 1.4420432220039292,
      "grad_norm": 2.033296585083008,
      "learning_rate": 1.0388998035363457e-05,
      "loss": 1.1477,
      "step": 3670
    },
    {
      "epoch": 1.4459724950884087,
      "grad_norm": 2.080641508102417,
      "learning_rate": 1.036280288146693e-05,
      "loss": 1.052,
      "step": 3680
    },
    {
      "epoch": 1.449901768172888,
      "grad_norm": 2.211304187774658,
      "learning_rate": 1.03366077275704e-05,
      "loss": 1.1059,
      "step": 3690
    },
    {
      "epoch": 1.4538310412573674,
      "grad_norm": 2.0872833728790283,
      "learning_rate": 1.0310412573673872e-05,
      "loss": 1.1055,
      "step": 3700
    },
    {
      "epoch": 1.4577603143418467,
      "grad_norm": 1.900280237197876,
      "learning_rate": 1.0284217419777342e-05,
      "loss": 1.1253,
      "step": 3710
    },
    {
      "epoch": 1.461689587426326,
      "grad_norm": 1.7630895376205444,
      "learning_rate": 1.0258022265880814e-05,
      "loss": 1.0691,
      "step": 3720
    },
    {
      "epoch": 1.4656188605108054,
      "grad_norm": 1.8622556924819946,
      "learning_rate": 1.0231827111984283e-05,
      "loss": 1.1491,
      "step": 3730
    },
    {
      "epoch": 1.469548133595285,
      "grad_norm": 2.0017082691192627,
      "learning_rate": 1.0205631958087755e-05,
      "loss": 1.0902,
      "step": 3740
    },
    {
      "epoch": 1.4734774066797642,
      "grad_norm": 1.8038382530212402,
      "learning_rate": 1.0179436804191225e-05,
      "loss": 1.0774,
      "step": 3750
    },
    {
      "epoch": 1.4774066797642436,
      "grad_norm": 1.7874146699905396,
      "learning_rate": 1.0153241650294697e-05,
      "loss": 1.0939,
      "step": 3760
    },
    {
      "epoch": 1.481335952848723,
      "grad_norm": 1.8896082639694214,
      "learning_rate": 1.0127046496398167e-05,
      "loss": 1.1054,
      "step": 3770
    },
    {
      "epoch": 1.4852652259332024,
      "grad_norm": 2.0804672241210938,
      "learning_rate": 1.010085134250164e-05,
      "loss": 1.1667,
      "step": 3780
    },
    {
      "epoch": 1.4891944990176817,
      "grad_norm": 1.855589747428894,
      "learning_rate": 1.007465618860511e-05,
      "loss": 1.1593,
      "step": 3790
    },
    {
      "epoch": 1.493123772102161,
      "grad_norm": 2.1032278537750244,
      "learning_rate": 1.0048461034708578e-05,
      "loss": 1.066,
      "step": 3800
    },
    {
      "epoch": 1.4970530451866404,
      "grad_norm": 2.3264729976654053,
      "learning_rate": 1.002226588081205e-05,
      "loss": 1.122,
      "step": 3810
    },
    {
      "epoch": 1.5009823182711197,
      "grad_norm": 1.6837120056152344,
      "learning_rate": 9.99607072691552e-06,
      "loss": 1.0772,
      "step": 3820
    },
    {
      "epoch": 1.5049115913555993,
      "grad_norm": 2.338047981262207,
      "learning_rate": 9.969875573018992e-06,
      "loss": 1.1197,
      "step": 3830
    },
    {
      "epoch": 1.5088408644400786,
      "grad_norm": 1.8639988899230957,
      "learning_rate": 9.943680419122463e-06,
      "loss": 1.0847,
      "step": 3840
    },
    {
      "epoch": 1.512770137524558,
      "grad_norm": 1.8722243309020996,
      "learning_rate": 9.917485265225933e-06,
      "loss": 1.1565,
      "step": 3850
    },
    {
      "epoch": 1.5166994106090375,
      "grad_norm": 1.973264455795288,
      "learning_rate": 9.891290111329405e-06,
      "loss": 1.124,
      "step": 3860
    },
    {
      "epoch": 1.5206286836935168,
      "grad_norm": 1.8991814851760864,
      "learning_rate": 9.865094957432875e-06,
      "loss": 1.1272,
      "step": 3870
    },
    {
      "epoch": 1.524557956777996,
      "grad_norm": 1.8213070631027222,
      "learning_rate": 9.838899803536347e-06,
      "loss": 1.0936,
      "step": 3880
    },
    {
      "epoch": 1.5284872298624754,
      "grad_norm": 2.33482027053833,
      "learning_rate": 9.812704649639818e-06,
      "loss": 1.095,
      "step": 3890
    },
    {
      "epoch": 1.5324165029469548,
      "grad_norm": 1.8462339639663696,
      "learning_rate": 9.786509495743288e-06,
      "loss": 1.1102,
      "step": 3900
    },
    {
      "epoch": 1.536345776031434,
      "grad_norm": 2.126552104949951,
      "learning_rate": 9.76031434184676e-06,
      "loss": 1.1322,
      "step": 3910
    },
    {
      "epoch": 1.5402750491159134,
      "grad_norm": 2.0181725025177,
      "learning_rate": 9.73411918795023e-06,
      "loss": 1.0986,
      "step": 3920
    },
    {
      "epoch": 1.544204322200393,
      "grad_norm": 2.09437894821167,
      "learning_rate": 9.7079240340537e-06,
      "loss": 1.1863,
      "step": 3930
    },
    {
      "epoch": 1.5481335952848723,
      "grad_norm": 2.0659472942352295,
      "learning_rate": 9.681728880157173e-06,
      "loss": 1.1734,
      "step": 3940
    },
    {
      "epoch": 1.5520628683693518,
      "grad_norm": 2.1383185386657715,
      "learning_rate": 9.655533726260643e-06,
      "loss": 1.1065,
      "step": 3950
    },
    {
      "epoch": 1.5559921414538311,
      "grad_norm": 2.074582576751709,
      "learning_rate": 9.629338572364113e-06,
      "loss": 1.1097,
      "step": 3960
    },
    {
      "epoch": 1.5599214145383105,
      "grad_norm": 2.2363107204437256,
      "learning_rate": 9.603143418467584e-06,
      "loss": 1.1685,
      "step": 3970
    },
    {
      "epoch": 1.5638506876227898,
      "grad_norm": 2.0122287273406982,
      "learning_rate": 9.576948264571056e-06,
      "loss": 1.0886,
      "step": 3980
    },
    {
      "epoch": 1.567779960707269,
      "grad_norm": 1.9452235698699951,
      "learning_rate": 9.550753110674526e-06,
      "loss": 1.1631,
      "step": 3990
    },
    {
      "epoch": 1.5717092337917484,
      "grad_norm": 1.9606884717941284,
      "learning_rate": 9.524557956777996e-06,
      "loss": 1.1295,
      "step": 4000
    },
    {
      "epoch": 1.5756385068762278,
      "grad_norm": 2.1677627563476562,
      "learning_rate": 9.498362802881468e-06,
      "loss": 1.0926,
      "step": 4010
    },
    {
      "epoch": 1.5795677799607073,
      "grad_norm": 2.4039816856384277,
      "learning_rate": 9.472167648984938e-06,
      "loss": 1.1462,
      "step": 4020
    },
    {
      "epoch": 1.5834970530451866,
      "grad_norm": 1.7657688856124878,
      "learning_rate": 9.445972495088409e-06,
      "loss": 1.1214,
      "step": 4030
    },
    {
      "epoch": 1.5874263261296662,
      "grad_norm": 1.7293751239776611,
      "learning_rate": 9.41977734119188e-06,
      "loss": 1.101,
      "step": 4040
    },
    {
      "epoch": 1.5913555992141455,
      "grad_norm": 2.3110859394073486,
      "learning_rate": 9.393582187295351e-06,
      "loss": 1.1822,
      "step": 4050
    },
    {
      "epoch": 1.5952848722986248,
      "grad_norm": 1.8584659099578857,
      "learning_rate": 9.367387033398821e-06,
      "loss": 1.1331,
      "step": 4060
    },
    {
      "epoch": 1.5992141453831041,
      "grad_norm": 2.035435199737549,
      "learning_rate": 9.341191879502293e-06,
      "loss": 1.1236,
      "step": 4070
    },
    {
      "epoch": 1.6031434184675835,
      "grad_norm": 1.7697163820266724,
      "learning_rate": 9.314996725605764e-06,
      "loss": 1.1231,
      "step": 4080
    },
    {
      "epoch": 1.6070726915520628,
      "grad_norm": 1.8251644372940063,
      "learning_rate": 9.288801571709236e-06,
      "loss": 1.1244,
      "step": 4090
    },
    {
      "epoch": 1.611001964636542,
      "grad_norm": 2.006316900253296,
      "learning_rate": 9.262606417812704e-06,
      "loss": 1.1114,
      "step": 4100
    },
    {
      "epoch": 1.6149312377210217,
      "grad_norm": 1.7989299297332764,
      "learning_rate": 9.236411263916176e-06,
      "loss": 1.1206,
      "step": 4110
    },
    {
      "epoch": 1.618860510805501,
      "grad_norm": 2.188478708267212,
      "learning_rate": 9.210216110019647e-06,
      "loss": 1.0866,
      "step": 4120
    },
    {
      "epoch": 1.6227897838899805,
      "grad_norm": 1.9883617162704468,
      "learning_rate": 9.184020956123117e-06,
      "loss": 1.0762,
      "step": 4130
    },
    {
      "epoch": 1.6267190569744598,
      "grad_norm": 2.3654074668884277,
      "learning_rate": 9.157825802226589e-06,
      "loss": 1.131,
      "step": 4140
    },
    {
      "epoch": 1.6306483300589392,
      "grad_norm": 1.8474254608154297,
      "learning_rate": 9.13163064833006e-06,
      "loss": 1.1151,
      "step": 4150
    },
    {
      "epoch": 1.6345776031434185,
      "grad_norm": 1.9643194675445557,
      "learning_rate": 9.105435494433531e-06,
      "loss": 1.1352,
      "step": 4160
    },
    {
      "epoch": 1.6385068762278978,
      "grad_norm": 2.0044097900390625,
      "learning_rate": 9.079240340537002e-06,
      "loss": 1.0961,
      "step": 4170
    },
    {
      "epoch": 1.6424361493123771,
      "grad_norm": 2.2462997436523438,
      "learning_rate": 9.053045186640472e-06,
      "loss": 1.1381,
      "step": 4180
    },
    {
      "epoch": 1.6463654223968565,
      "grad_norm": 1.9525291919708252,
      "learning_rate": 9.026850032743944e-06,
      "loss": 1.1351,
      "step": 4190
    },
    {
      "epoch": 1.650294695481336,
      "grad_norm": 2.103827476501465,
      "learning_rate": 9.000654878847414e-06,
      "loss": 1.126,
      "step": 4200
    },
    {
      "epoch": 1.6542239685658153,
      "grad_norm": 2.320218563079834,
      "learning_rate": 8.974459724950884e-06,
      "loss": 1.1237,
      "step": 4210
    },
    {
      "epoch": 1.6581532416502947,
      "grad_norm": 2.3648152351379395,
      "learning_rate": 8.948264571054357e-06,
      "loss": 1.1038,
      "step": 4220
    },
    {
      "epoch": 1.6620825147347742,
      "grad_norm": 1.8734126091003418,
      "learning_rate": 8.922069417157827e-06,
      "loss": 1.1465,
      "step": 4230
    },
    {
      "epoch": 1.6660117878192535,
      "grad_norm": 2.036123752593994,
      "learning_rate": 8.895874263261297e-06,
      "loss": 1.1532,
      "step": 4240
    },
    {
      "epoch": 1.6699410609037328,
      "grad_norm": 2.3580853939056396,
      "learning_rate": 8.869679109364769e-06,
      "loss": 1.1774,
      "step": 4250
    },
    {
      "epoch": 1.6738703339882122,
      "grad_norm": 1.905924916267395,
      "learning_rate": 8.84348395546824e-06,
      "loss": 1.1434,
      "step": 4260
    },
    {
      "epoch": 1.6777996070726915,
      "grad_norm": 2.327120780944824,
      "learning_rate": 8.81728880157171e-06,
      "loss": 1.0944,
      "step": 4270
    },
    {
      "epoch": 1.6817288801571708,
      "grad_norm": 1.7689050436019897,
      "learning_rate": 8.79109364767518e-06,
      "loss": 1.1429,
      "step": 4280
    },
    {
      "epoch": 1.6856581532416501,
      "grad_norm": 2.397066354751587,
      "learning_rate": 8.764898493778652e-06,
      "loss": 1.0998,
      "step": 4290
    },
    {
      "epoch": 1.6895874263261297,
      "grad_norm": 1.698534369468689,
      "learning_rate": 8.738703339882122e-06,
      "loss": 1.1433,
      "step": 4300
    },
    {
      "epoch": 1.693516699410609,
      "grad_norm": 1.9618228673934937,
      "learning_rate": 8.712508185985593e-06,
      "loss": 1.0925,
      "step": 4310
    },
    {
      "epoch": 1.6974459724950886,
      "grad_norm": 2.091972827911377,
      "learning_rate": 8.686313032089065e-06,
      "loss": 1.0733,
      "step": 4320
    },
    {
      "epoch": 1.7013752455795679,
      "grad_norm": 1.9837088584899902,
      "learning_rate": 8.660117878192535e-06,
      "loss": 1.0974,
      "step": 4330
    },
    {
      "epoch": 1.7053045186640472,
      "grad_norm": 1.8950345516204834,
      "learning_rate": 8.633922724296005e-06,
      "loss": 1.1037,
      "step": 4340
    },
    {
      "epoch": 1.7092337917485265,
      "grad_norm": 2.46164870262146,
      "learning_rate": 8.607727570399477e-06,
      "loss": 1.139,
      "step": 4350
    },
    {
      "epoch": 1.7131630648330058,
      "grad_norm": 1.9857279062271118,
      "learning_rate": 8.581532416502948e-06,
      "loss": 1.136,
      "step": 4360
    },
    {
      "epoch": 1.7170923379174852,
      "grad_norm": 2.4330341815948486,
      "learning_rate": 8.55533726260642e-06,
      "loss": 1.1134,
      "step": 4370
    },
    {
      "epoch": 1.7210216110019645,
      "grad_norm": 2.3502864837646484,
      "learning_rate": 8.52914210870989e-06,
      "loss": 1.0838,
      "step": 4380
    },
    {
      "epoch": 1.724950884086444,
      "grad_norm": 1.569015622138977,
      "learning_rate": 8.50294695481336e-06,
      "loss": 1.1333,
      "step": 4390
    },
    {
      "epoch": 1.7288801571709234,
      "grad_norm": 2.056370496749878,
      "learning_rate": 8.476751800916832e-06,
      "loss": 1.1275,
      "step": 4400
    },
    {
      "epoch": 1.732809430255403,
      "grad_norm": 2.0674333572387695,
      "learning_rate": 8.4505566470203e-06,
      "loss": 1.1286,
      "step": 4410
    },
    {
      "epoch": 1.7367387033398822,
      "grad_norm": 1.9675507545471191,
      "learning_rate": 8.424361493123773e-06,
      "loss": 1.1399,
      "step": 4420
    },
    {
      "epoch": 1.7406679764243616,
      "grad_norm": 1.7851428985595703,
      "learning_rate": 8.398166339227243e-06,
      "loss": 1.1194,
      "step": 4430
    },
    {
      "epoch": 1.7445972495088409,
      "grad_norm": 1.9123866558074951,
      "learning_rate": 8.371971185330713e-06,
      "loss": 1.1762,
      "step": 4440
    },
    {
      "epoch": 1.7485265225933202,
      "grad_norm": 1.9975557327270508,
      "learning_rate": 8.345776031434185e-06,
      "loss": 1.0458,
      "step": 4450
    },
    {
      "epoch": 1.7524557956777995,
      "grad_norm": 2.120887279510498,
      "learning_rate": 8.319580877537656e-06,
      "loss": 1.0663,
      "step": 4460
    },
    {
      "epoch": 1.7563850687622788,
      "grad_norm": 2.3891496658325195,
      "learning_rate": 8.293385723641128e-06,
      "loss": 1.158,
      "step": 4470
    },
    {
      "epoch": 1.7603143418467584,
      "grad_norm": 1.9473938941955566,
      "learning_rate": 8.267190569744598e-06,
      "loss": 1.087,
      "step": 4480
    },
    {
      "epoch": 1.7642436149312377,
      "grad_norm": 2.2114155292510986,
      "learning_rate": 8.240995415848068e-06,
      "loss": 1.1585,
      "step": 4490
    },
    {
      "epoch": 1.768172888015717,
      "grad_norm": 2.052994966506958,
      "learning_rate": 8.21480026195154e-06,
      "loss": 1.1418,
      "step": 4500
    },
    {
      "epoch": 1.7721021611001966,
      "grad_norm": 2.534231662750244,
      "learning_rate": 8.18860510805501e-06,
      "loss": 1.142,
      "step": 4510
    },
    {
      "epoch": 1.776031434184676,
      "grad_norm": 1.7697168588638306,
      "learning_rate": 8.162409954158481e-06,
      "loss": 1.1207,
      "step": 4520
    },
    {
      "epoch": 1.7799607072691552,
      "grad_norm": 2.074934720993042,
      "learning_rate": 8.136214800261953e-06,
      "loss": 1.0779,
      "step": 4530
    },
    {
      "epoch": 1.7838899803536346,
      "grad_norm": 2.472543239593506,
      "learning_rate": 8.110019646365423e-06,
      "loss": 1.074,
      "step": 4540
    },
    {
      "epoch": 1.7878192534381139,
      "grad_norm": 2.5953056812286377,
      "learning_rate": 8.083824492468894e-06,
      "loss": 1.0582,
      "step": 4550
    },
    {
      "epoch": 1.7917485265225932,
      "grad_norm": 2.0445704460144043,
      "learning_rate": 8.057629338572364e-06,
      "loss": 1.06,
      "step": 4560
    },
    {
      "epoch": 1.7956777996070727,
      "grad_norm": 1.7268553972244263,
      "learning_rate": 8.031434184675836e-06,
      "loss": 1.1131,
      "step": 4570
    },
    {
      "epoch": 1.799607072691552,
      "grad_norm": 1.8097492456436157,
      "learning_rate": 8.005239030779306e-06,
      "loss": 1.1234,
      "step": 4580
    },
    {
      "epoch": 1.8035363457760314,
      "grad_norm": 2.0219483375549316,
      "learning_rate": 7.979043876882777e-06,
      "loss": 1.0971,
      "step": 4590
    },
    {
      "epoch": 1.807465618860511,
      "grad_norm": 2.2048354148864746,
      "learning_rate": 7.952848722986249e-06,
      "loss": 1.1113,
      "step": 4600
    },
    {
      "epoch": 1.8113948919449903,
      "grad_norm": 1.9870160818099976,
      "learning_rate": 7.926653569089719e-06,
      "loss": 1.1367,
      "step": 4610
    },
    {
      "epoch": 1.8153241650294696,
      "grad_norm": 1.743205189704895,
      "learning_rate": 7.90045841519319e-06,
      "loss": 1.103,
      "step": 4620
    },
    {
      "epoch": 1.819253438113949,
      "grad_norm": 1.9095025062561035,
      "learning_rate": 7.874263261296661e-06,
      "loss": 1.158,
      "step": 4630
    },
    {
      "epoch": 1.8231827111984282,
      "grad_norm": 2.148283004760742,
      "learning_rate": 7.848068107400131e-06,
      "loss": 1.1616,
      "step": 4640
    },
    {
      "epoch": 1.8271119842829076,
      "grad_norm": 2.399543046951294,
      "learning_rate": 7.821872953503602e-06,
      "loss": 1.1041,
      "step": 4650
    },
    {
      "epoch": 1.8310412573673869,
      "grad_norm": 2.2009127140045166,
      "learning_rate": 7.795677799607074e-06,
      "loss": 1.1385,
      "step": 4660
    },
    {
      "epoch": 1.8349705304518664,
      "grad_norm": 2.490344762802124,
      "learning_rate": 7.769482645710544e-06,
      "loss": 1.0936,
      "step": 4670
    },
    {
      "epoch": 1.8388998035363457,
      "grad_norm": 1.94547438621521,
      "learning_rate": 7.743287491814016e-06,
      "loss": 1.1289,
      "step": 4680
    },
    {
      "epoch": 1.8428290766208253,
      "grad_norm": 2.2545294761657715,
      "learning_rate": 7.717092337917485e-06,
      "loss": 1.0791,
      "step": 4690
    },
    {
      "epoch": 1.8467583497053046,
      "grad_norm": 2.277522087097168,
      "learning_rate": 7.690897184020957e-06,
      "loss": 1.1043,
      "step": 4700
    },
    {
      "epoch": 1.850687622789784,
      "grad_norm": 1.9023038148880005,
      "learning_rate": 7.664702030124429e-06,
      "loss": 1.2039,
      "step": 4710
    },
    {
      "epoch": 1.8546168958742633,
      "grad_norm": 2.111762285232544,
      "learning_rate": 7.638506876227897e-06,
      "loss": 1.1378,
      "step": 4720
    },
    {
      "epoch": 1.8585461689587426,
      "grad_norm": 2.0611770153045654,
      "learning_rate": 7.612311722331369e-06,
      "loss": 1.1239,
      "step": 4730
    },
    {
      "epoch": 1.862475442043222,
      "grad_norm": 2.47686505317688,
      "learning_rate": 7.5861165684348405e-06,
      "loss": 1.1698,
      "step": 4740
    },
    {
      "epoch": 1.8664047151277012,
      "grad_norm": 1.8377643823623657,
      "learning_rate": 7.559921414538312e-06,
      "loss": 1.0973,
      "step": 4750
    },
    {
      "epoch": 1.8703339882121808,
      "grad_norm": 1.8283066749572754,
      "learning_rate": 7.533726260641782e-06,
      "loss": 1.1018,
      "step": 4760
    },
    {
      "epoch": 1.87426326129666,
      "grad_norm": 2.2058913707733154,
      "learning_rate": 7.507531106745253e-06,
      "loss": 1.0984,
      "step": 4770
    },
    {
      "epoch": 1.8781925343811396,
      "grad_norm": 2.620028018951416,
      "learning_rate": 7.481335952848724e-06,
      "loss": 1.1702,
      "step": 4780
    },
    {
      "epoch": 1.882121807465619,
      "grad_norm": 2.1953916549682617,
      "learning_rate": 7.455140798952194e-06,
      "loss": 1.1314,
      "step": 4790
    },
    {
      "epoch": 1.8860510805500983,
      "grad_norm": 2.1894943714141846,
      "learning_rate": 7.428945645055665e-06,
      "loss": 1.1255,
      "step": 4800
    },
    {
      "epoch": 1.8899803536345776,
      "grad_norm": 2.126819133758545,
      "learning_rate": 7.402750491159136e-06,
      "loss": 1.1469,
      "step": 4810
    },
    {
      "epoch": 1.893909626719057,
      "grad_norm": 2.055104970932007,
      "learning_rate": 7.376555337262607e-06,
      "loss": 1.0822,
      "step": 4820
    },
    {
      "epoch": 1.8978388998035363,
      "grad_norm": 1.8257137537002563,
      "learning_rate": 7.3503601833660775e-06,
      "loss": 1.1287,
      "step": 4830
    },
    {
      "epoch": 1.9017681728880156,
      "grad_norm": 2.0118353366851807,
      "learning_rate": 7.324165029469549e-06,
      "loss": 1.132,
      "step": 4840
    },
    {
      "epoch": 1.9056974459724951,
      "grad_norm": 2.0273349285125732,
      "learning_rate": 7.29796987557302e-06,
      "loss": 1.1336,
      "step": 4850
    },
    {
      "epoch": 1.9096267190569745,
      "grad_norm": 2.667896270751953,
      "learning_rate": 7.27177472167649e-06,
      "loss": 1.082,
      "step": 4860
    },
    {
      "epoch": 1.9135559921414538,
      "grad_norm": 1.9339599609375,
      "learning_rate": 7.245579567779961e-06,
      "loss": 1.118,
      "step": 4870
    },
    {
      "epoch": 1.9174852652259333,
      "grad_norm": 2.323564052581787,
      "learning_rate": 7.2193844138834325e-06,
      "loss": 1.1166,
      "step": 4880
    },
    {
      "epoch": 1.9214145383104126,
      "grad_norm": 1.9730135202407837,
      "learning_rate": 7.193189259986904e-06,
      "loss": 1.0683,
      "step": 4890
    },
    {
      "epoch": 1.925343811394892,
      "grad_norm": 1.932449221611023,
      "learning_rate": 7.166994106090374e-06,
      "loss": 1.1179,
      "step": 4900
    },
    {
      "epoch": 1.9292730844793713,
      "grad_norm": 2.174551010131836,
      "learning_rate": 7.140798952193845e-06,
      "loss": 1.1776,
      "step": 4910
    },
    {
      "epoch": 1.9332023575638506,
      "grad_norm": 2.2572109699249268,
      "learning_rate": 7.114603798297316e-06,
      "loss": 1.1352,
      "step": 4920
    },
    {
      "epoch": 1.93713163064833,
      "grad_norm": 2.061349391937256,
      "learning_rate": 7.088408644400786e-06,
      "loss": 1.1213,
      "step": 4930
    },
    {
      "epoch": 1.9410609037328095,
      "grad_norm": 2.119699478149414,
      "learning_rate": 7.062213490504257e-06,
      "loss": 1.0945,
      "step": 4940
    },
    {
      "epoch": 1.9449901768172888,
      "grad_norm": 2.7368078231811523,
      "learning_rate": 7.036018336607728e-06,
      "loss": 1.144,
      "step": 4950
    },
    {
      "epoch": 1.9489194499017681,
      "grad_norm": 1.7057870626449585,
      "learning_rate": 7.009823182711199e-06,
      "loss": 1.0784,
      "step": 4960
    },
    {
      "epoch": 1.9528487229862477,
      "grad_norm": 1.9970049858093262,
      "learning_rate": 6.9836280288146695e-06,
      "loss": 1.1811,
      "step": 4970
    },
    {
      "epoch": 1.956777996070727,
      "grad_norm": 2.181785821914673,
      "learning_rate": 6.957432874918141e-06,
      "loss": 1.1364,
      "step": 4980
    },
    {
      "epoch": 1.9607072691552063,
      "grad_norm": 2.004328966140747,
      "learning_rate": 6.931237721021612e-06,
      "loss": 1.1503,
      "step": 4990
    },
    {
      "epoch": 1.9646365422396856,
      "grad_norm": 2.1303439140319824,
      "learning_rate": 6.905042567125082e-06,
      "loss": 1.116,
      "step": 5000
    },
    {
      "epoch": 1.968565815324165,
      "grad_norm": 1.8391004800796509,
      "learning_rate": 6.878847413228553e-06,
      "loss": 1.1382,
      "step": 5010
    },
    {
      "epoch": 1.9724950884086443,
      "grad_norm": 1.96843421459198,
      "learning_rate": 6.852652259332024e-06,
      "loss": 1.1504,
      "step": 5020
    },
    {
      "epoch": 1.9764243614931236,
      "grad_norm": 2.054025411605835,
      "learning_rate": 6.826457105435495e-06,
      "loss": 1.1515,
      "step": 5030
    },
    {
      "epoch": 1.9803536345776032,
      "grad_norm": 2.140714406967163,
      "learning_rate": 6.800261951538966e-06,
      "loss": 1.1824,
      "step": 5040
    },
    {
      "epoch": 1.9842829076620825,
      "grad_norm": 2.2235193252563477,
      "learning_rate": 6.774066797642437e-06,
      "loss": 1.1289,
      "step": 5050
    },
    {
      "epoch": 1.988212180746562,
      "grad_norm": 2.1113593578338623,
      "learning_rate": 6.747871643745908e-06,
      "loss": 1.113,
      "step": 5060
    },
    {
      "epoch": 1.9921414538310414,
      "grad_norm": 2.1347200870513916,
      "learning_rate": 6.721676489849378e-06,
      "loss": 1.1545,
      "step": 5070
    },
    {
      "epoch": 1.9960707269155207,
      "grad_norm": 2.266566038131714,
      "learning_rate": 6.695481335952849e-06,
      "loss": 1.1554,
      "step": 5080
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.7868313789367676,
      "learning_rate": 6.669286182056321e-06,
      "loss": 1.1277,
      "step": 5090
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.122260332107544,
      "eval_runtime": 50.6758,
      "eval_samples_per_second": 25.12,
      "eval_steps_per_second": 6.295,
      "step": 5090
    }
  ],
  "logging_steps": 10,
  "max_steps": 7635,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.476225460358349e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
