{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 2700,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011111111111111112,
      "grad_norm": 0.7498253583908081,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 3.3846,
      "step": 10
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 0.8265495300292969,
      "learning_rate": 1.985925925925926e-05,
      "loss": 3.3411,
      "step": 20
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.9576471447944641,
      "learning_rate": 1.9785185185185187e-05,
      "loss": 3.3258,
      "step": 30
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 0.9061447978019714,
      "learning_rate": 1.971111111111111e-05,
      "loss": 3.1666,
      "step": 40
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 0.9103816151618958,
      "learning_rate": 1.963703703703704e-05,
      "loss": 3.1448,
      "step": 50
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.9098743200302124,
      "learning_rate": 1.9562962962962964e-05,
      "loss": 2.9866,
      "step": 60
    },
    {
      "epoch": 0.07777777777777778,
      "grad_norm": 1.1365054845809937,
      "learning_rate": 1.948888888888889e-05,
      "loss": 2.8827,
      "step": 70
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 1.126818299293518,
      "learning_rate": 1.9414814814814817e-05,
      "loss": 2.8532,
      "step": 80
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.2563832998275757,
      "learning_rate": 1.9340740740740743e-05,
      "loss": 2.817,
      "step": 90
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 1.217354655265808,
      "learning_rate": 1.926666666666667e-05,
      "loss": 2.6358,
      "step": 100
    },
    {
      "epoch": 0.12222222222222222,
      "grad_norm": 1.3533070087432861,
      "learning_rate": 1.9192592592592593e-05,
      "loss": 2.5876,
      "step": 110
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 1.3956297636032104,
      "learning_rate": 1.911851851851852e-05,
      "loss": 2.502,
      "step": 120
    },
    {
      "epoch": 0.14444444444444443,
      "grad_norm": 1.3414335250854492,
      "learning_rate": 1.9044444444444446e-05,
      "loss": 2.2946,
      "step": 130
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 1.5750452280044556,
      "learning_rate": 1.8970370370370372e-05,
      "loss": 2.3194,
      "step": 140
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.6017978191375732,
      "learning_rate": 1.8896296296296295e-05,
      "loss": 2.3104,
      "step": 150
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 1.7502506971359253,
      "learning_rate": 1.8822222222222225e-05,
      "loss": 2.093,
      "step": 160
    },
    {
      "epoch": 0.18888888888888888,
      "grad_norm": 1.8226639032363892,
      "learning_rate": 1.874814814814815e-05,
      "loss": 2.0539,
      "step": 170
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.8674681186676025,
      "learning_rate": 1.8674074074074075e-05,
      "loss": 1.9511,
      "step": 180
    },
    {
      "epoch": 0.2111111111111111,
      "grad_norm": 1.902590036392212,
      "learning_rate": 1.86e-05,
      "loss": 1.7988,
      "step": 190
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 2.0430397987365723,
      "learning_rate": 1.8525925925925928e-05,
      "loss": 1.4862,
      "step": 200
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 4.112367153167725,
      "learning_rate": 1.8451851851851855e-05,
      "loss": 1.4848,
      "step": 210
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 2.4387574195861816,
      "learning_rate": 1.8377777777777778e-05,
      "loss": 1.509,
      "step": 220
    },
    {
      "epoch": 0.25555555555555554,
      "grad_norm": 1.979012131690979,
      "learning_rate": 1.8303703703703704e-05,
      "loss": 1.3405,
      "step": 230
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.737240195274353,
      "learning_rate": 1.822962962962963e-05,
      "loss": 1.2069,
      "step": 240
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 2.6330318450927734,
      "learning_rate": 1.8155555555555557e-05,
      "loss": 1.3355,
      "step": 250
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 1.9576369524002075,
      "learning_rate": 1.8081481481481484e-05,
      "loss": 1.1217,
      "step": 260
    },
    {
      "epoch": 0.3,
      "grad_norm": 2.480271339416504,
      "learning_rate": 1.800740740740741e-05,
      "loss": 1.2478,
      "step": 270
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 1.862147569656372,
      "learning_rate": 1.7933333333333333e-05,
      "loss": 1.1292,
      "step": 280
    },
    {
      "epoch": 0.32222222222222224,
      "grad_norm": 1.858933925628662,
      "learning_rate": 1.785925925925926e-05,
      "loss": 0.9878,
      "step": 290
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.8563158512115479,
      "learning_rate": 1.7785185185185186e-05,
      "loss": 0.8836,
      "step": 300
    },
    {
      "epoch": 0.34444444444444444,
      "grad_norm": 1.4641475677490234,
      "learning_rate": 1.7711111111111113e-05,
      "loss": 0.8414,
      "step": 310
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 2.001438617706299,
      "learning_rate": 1.763703703703704e-05,
      "loss": 0.8856,
      "step": 320
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 1.7462308406829834,
      "learning_rate": 1.7562962962962962e-05,
      "loss": 0.8715,
      "step": 330
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 2.1090571880340576,
      "learning_rate": 1.7488888888888892e-05,
      "loss": 0.9363,
      "step": 340
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 1.4150822162628174,
      "learning_rate": 1.7414814814814815e-05,
      "loss": 0.8022,
      "step": 350
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.4774588346481323,
      "learning_rate": 1.7340740740740742e-05,
      "loss": 0.7147,
      "step": 360
    },
    {
      "epoch": 0.4111111111111111,
      "grad_norm": 1.9402284622192383,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.8105,
      "step": 370
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 1.3352043628692627,
      "learning_rate": 1.7192592592592595e-05,
      "loss": 0.7473,
      "step": 380
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 1.8826024532318115,
      "learning_rate": 1.711851851851852e-05,
      "loss": 0.7763,
      "step": 390
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.2831512689590454,
      "learning_rate": 1.7044444444444445e-05,
      "loss": 0.7372,
      "step": 400
    },
    {
      "epoch": 0.45555555555555555,
      "grad_norm": 1.7004103660583496,
      "learning_rate": 1.697037037037037e-05,
      "loss": 0.8736,
      "step": 410
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 1.4687162637710571,
      "learning_rate": 1.6896296296296298e-05,
      "loss": 0.6245,
      "step": 420
    },
    {
      "epoch": 0.4777777777777778,
      "grad_norm": 2.245051383972168,
      "learning_rate": 1.6822222222222224e-05,
      "loss": 0.6758,
      "step": 430
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 1.505418062210083,
      "learning_rate": 1.6748148148148147e-05,
      "loss": 0.6186,
      "step": 440
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.1917393207550049,
      "learning_rate": 1.6674074074074077e-05,
      "loss": 0.663,
      "step": 450
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 1.3897839784622192,
      "learning_rate": 1.66e-05,
      "loss": 0.6998,
      "step": 460
    },
    {
      "epoch": 0.5222222222222223,
      "grad_norm": 1.0970280170440674,
      "learning_rate": 1.6525925925925927e-05,
      "loss": 0.5536,
      "step": 470
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 1.0683971643447876,
      "learning_rate": 1.6451851851851853e-05,
      "loss": 0.6021,
      "step": 480
    },
    {
      "epoch": 0.5444444444444444,
      "grad_norm": 1.4584951400756836,
      "learning_rate": 1.637777777777778e-05,
      "loss": 0.6486,
      "step": 490
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 1.5495928525924683,
      "learning_rate": 1.6303703703703706e-05,
      "loss": 0.6377,
      "step": 500
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 1.3442260026931763,
      "learning_rate": 1.622962962962963e-05,
      "loss": 0.5729,
      "step": 510
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 1.4791569709777832,
      "learning_rate": 1.6155555555555556e-05,
      "loss": 0.6322,
      "step": 520
    },
    {
      "epoch": 0.5888888888888889,
      "grad_norm": 0.9283771514892578,
      "learning_rate": 1.6081481481481482e-05,
      "loss": 0.6209,
      "step": 530
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.3184871673583984,
      "learning_rate": 1.600740740740741e-05,
      "loss": 0.5536,
      "step": 540
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 1.3300087451934814,
      "learning_rate": 1.5933333333333336e-05,
      "loss": 0.5816,
      "step": 550
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 1.140045166015625,
      "learning_rate": 1.5859259259259262e-05,
      "loss": 0.5966,
      "step": 560
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 1.0008349418640137,
      "learning_rate": 1.5785185185185185e-05,
      "loss": 0.6193,
      "step": 570
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 1.4760969877243042,
      "learning_rate": 1.571111111111111e-05,
      "loss": 0.6372,
      "step": 580
    },
    {
      "epoch": 0.6555555555555556,
      "grad_norm": 1.3532527685165405,
      "learning_rate": 1.5637037037037038e-05,
      "loss": 0.695,
      "step": 590
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.8245619535446167,
      "learning_rate": 1.5562962962962965e-05,
      "loss": 0.7424,
      "step": 600
    },
    {
      "epoch": 0.6777777777777778,
      "grad_norm": 1.4853192567825317,
      "learning_rate": 1.548888888888889e-05,
      "loss": 0.5968,
      "step": 610
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 1.8561210632324219,
      "learning_rate": 1.5414814814814814e-05,
      "loss": 0.7222,
      "step": 620
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.8659898042678833,
      "learning_rate": 1.5340740740740744e-05,
      "loss": 0.6922,
      "step": 630
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 1.1301740407943726,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.5461,
      "step": 640
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 1.152784824371338,
      "learning_rate": 1.5192592592592594e-05,
      "loss": 0.6035,
      "step": 650
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 1.287928819656372,
      "learning_rate": 1.5118518518518519e-05,
      "loss": 0.5988,
      "step": 660
    },
    {
      "epoch": 0.7444444444444445,
      "grad_norm": 0.9236172437667847,
      "learning_rate": 1.5044444444444445e-05,
      "loss": 0.6477,
      "step": 670
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 1.1402990818023682,
      "learning_rate": 1.497037037037037e-05,
      "loss": 0.5675,
      "step": 680
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.9751750230789185,
      "learning_rate": 1.4896296296296298e-05,
      "loss": 0.579,
      "step": 690
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 1.408682942390442,
      "learning_rate": 1.4822222222222225e-05,
      "loss": 0.5451,
      "step": 700
    },
    {
      "epoch": 0.7888888888888889,
      "grad_norm": 1.6119753122329712,
      "learning_rate": 1.474814814814815e-05,
      "loss": 0.6072,
      "step": 710
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.814921498298645,
      "learning_rate": 1.4674074074074076e-05,
      "loss": 0.5085,
      "step": 720
    },
    {
      "epoch": 0.8111111111111111,
      "grad_norm": 0.9411621689796448,
      "learning_rate": 1.46e-05,
      "loss": 0.5904,
      "step": 730
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 1.0821774005889893,
      "learning_rate": 1.4525925925925927e-05,
      "loss": 0.5403,
      "step": 740
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 1.0005278587341309,
      "learning_rate": 1.4451851851851852e-05,
      "loss": 0.5576,
      "step": 750
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 0.9982365369796753,
      "learning_rate": 1.4377777777777779e-05,
      "loss": 0.5361,
      "step": 760
    },
    {
      "epoch": 0.8555555555555555,
      "grad_norm": 1.2921760082244873,
      "learning_rate": 1.4303703703703703e-05,
      "loss": 0.7134,
      "step": 770
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 1.17210853099823,
      "learning_rate": 1.4229629629629632e-05,
      "loss": 0.5011,
      "step": 780
    },
    {
      "epoch": 0.8777777777777778,
      "grad_norm": 1.4485975503921509,
      "learning_rate": 1.4155555555555556e-05,
      "loss": 0.7106,
      "step": 790
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.0276108980178833,
      "learning_rate": 1.4081481481481483e-05,
      "loss": 0.6206,
      "step": 800
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.4524993896484375,
      "learning_rate": 1.400740740740741e-05,
      "loss": 0.545,
      "step": 810
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 1.1412451267242432,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.624,
      "step": 820
    },
    {
      "epoch": 0.9222222222222223,
      "grad_norm": 0.8641870617866516,
      "learning_rate": 1.385925925925926e-05,
      "loss": 0.4872,
      "step": 830
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.0089077949523926,
      "learning_rate": 1.3785185185185186e-05,
      "loss": 0.5434,
      "step": 840
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 0.8221650123596191,
      "learning_rate": 1.3711111111111112e-05,
      "loss": 0.5609,
      "step": 850
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 1.102746605873108,
      "learning_rate": 1.3637037037037037e-05,
      "loss": 0.4976,
      "step": 860
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 1.1352027654647827,
      "learning_rate": 1.3562962962962965e-05,
      "loss": 0.595,
      "step": 870
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.7975687384605408,
      "learning_rate": 1.3488888888888888e-05,
      "loss": 0.5302,
      "step": 880
    },
    {
      "epoch": 0.9888888888888889,
      "grad_norm": 0.7771749496459961,
      "learning_rate": 1.3414814814814817e-05,
      "loss": 0.4601,
      "step": 890
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.0243685245513916,
      "learning_rate": 1.3340740740740741e-05,
      "loss": 0.5045,
      "step": 900
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.4956647753715515,
      "eval_runtime": 7.8402,
      "eval_samples_per_second": 12.755,
      "eval_steps_per_second": 12.755,
      "step": 900
    },
    {
      "epoch": 1.011111111111111,
      "grad_norm": 1.2668895721435547,
      "learning_rate": 1.3266666666666668e-05,
      "loss": 0.5862,
      "step": 910
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 0.7115220427513123,
      "learning_rate": 1.3192592592592594e-05,
      "loss": 0.5159,
      "step": 920
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 1.09931218624115,
      "learning_rate": 1.311851851851852e-05,
      "loss": 0.553,
      "step": 930
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 1.3489577770233154,
      "learning_rate": 1.3044444444444446e-05,
      "loss": 0.5255,
      "step": 940
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 2.4433093070983887,
      "learning_rate": 1.297037037037037e-05,
      "loss": 0.6388,
      "step": 950
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.8731195330619812,
      "learning_rate": 1.2896296296296299e-05,
      "loss": 0.6359,
      "step": 960
    },
    {
      "epoch": 1.0777777777777777,
      "grad_norm": 0.88451087474823,
      "learning_rate": 1.2822222222222222e-05,
      "loss": 0.539,
      "step": 970
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 0.8156394958496094,
      "learning_rate": 1.274814814814815e-05,
      "loss": 0.5376,
      "step": 980
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.9106233716011047,
      "learning_rate": 1.2674074074074075e-05,
      "loss": 0.6129,
      "step": 990
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 1.7998676300048828,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.5342,
      "step": 1000
    },
    {
      "epoch": 1.1222222222222222,
      "grad_norm": 0.7410822510719299,
      "learning_rate": 1.2525925925925928e-05,
      "loss": 0.4778,
      "step": 1010
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.7856020927429199,
      "learning_rate": 1.2451851851851853e-05,
      "loss": 0.4816,
      "step": 1020
    },
    {
      "epoch": 1.1444444444444444,
      "grad_norm": 1.011325716972351,
      "learning_rate": 1.237777777777778e-05,
      "loss": 0.5687,
      "step": 1030
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 1.0366287231445312,
      "learning_rate": 1.2303703703703704e-05,
      "loss": 0.6055,
      "step": 1040
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.6306378245353699,
      "learning_rate": 1.222962962962963e-05,
      "loss": 0.5376,
      "step": 1050
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 0.9887213706970215,
      "learning_rate": 1.2155555555555555e-05,
      "loss": 0.4529,
      "step": 1060
    },
    {
      "epoch": 1.1888888888888889,
      "grad_norm": 1.0510892868041992,
      "learning_rate": 1.2081481481481484e-05,
      "loss": 0.5474,
      "step": 1070
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.2708779573440552,
      "learning_rate": 1.2007407407407408e-05,
      "loss": 0.5902,
      "step": 1080
    },
    {
      "epoch": 1.211111111111111,
      "grad_norm": 1.3046174049377441,
      "learning_rate": 1.1933333333333335e-05,
      "loss": 0.612,
      "step": 1090
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.8899508714675903,
      "learning_rate": 1.185925925925926e-05,
      "loss": 0.5413,
      "step": 1100
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 1.2777204513549805,
      "learning_rate": 1.1785185185185186e-05,
      "loss": 0.6546,
      "step": 1110
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 0.899054765701294,
      "learning_rate": 1.1711111111111113e-05,
      "loss": 0.5243,
      "step": 1120
    },
    {
      "epoch": 1.2555555555555555,
      "grad_norm": 0.8266419768333435,
      "learning_rate": 1.1637037037037037e-05,
      "loss": 0.5064,
      "step": 1130
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 1.235256314277649,
      "learning_rate": 1.1562962962962964e-05,
      "loss": 0.4845,
      "step": 1140
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 0.9975486397743225,
      "learning_rate": 1.1488888888888889e-05,
      "loss": 0.5506,
      "step": 1150
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 1.2014371156692505,
      "learning_rate": 1.1414814814814817e-05,
      "loss": 0.6836,
      "step": 1160
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.3731484413146973,
      "learning_rate": 1.1340740740740742e-05,
      "loss": 0.628,
      "step": 1170
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 1.106218695640564,
      "learning_rate": 1.1266666666666668e-05,
      "loss": 0.6369,
      "step": 1180
    },
    {
      "epoch": 1.3222222222222222,
      "grad_norm": 0.6797612309455872,
      "learning_rate": 1.1192592592592593e-05,
      "loss": 0.6246,
      "step": 1190
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.9216657876968384,
      "learning_rate": 1.111851851851852e-05,
      "loss": 0.5579,
      "step": 1200
    },
    {
      "epoch": 1.3444444444444446,
      "grad_norm": 0.7170698642730713,
      "learning_rate": 1.1044444444444444e-05,
      "loss": 0.5297,
      "step": 1210
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 1.1444318294525146,
      "learning_rate": 1.0970370370370371e-05,
      "loss": 0.5794,
      "step": 1220
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 1.0209628343582153,
      "learning_rate": 1.0896296296296298e-05,
      "loss": 0.5597,
      "step": 1230
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 1.006665825843811,
      "learning_rate": 1.0822222222222222e-05,
      "loss": 0.5179,
      "step": 1240
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 0.868777871131897,
      "learning_rate": 1.074814814814815e-05,
      "loss": 0.5458,
      "step": 1250
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.8066248297691345,
      "learning_rate": 1.0674074074074074e-05,
      "loss": 0.4704,
      "step": 1260
    },
    {
      "epoch": 1.411111111111111,
      "grad_norm": 0.9042888283729553,
      "learning_rate": 1.0600000000000002e-05,
      "loss": 0.6083,
      "step": 1270
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 1.4074255228042603,
      "learning_rate": 1.0525925925925927e-05,
      "loss": 0.6192,
      "step": 1280
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.8854765892028809,
      "learning_rate": 1.0451851851851853e-05,
      "loss": 0.5391,
      "step": 1290
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.6635987758636475,
      "learning_rate": 1.0377777777777778e-05,
      "loss": 0.6409,
      "step": 1300
    },
    {
      "epoch": 1.4555555555555555,
      "grad_norm": 0.9972749948501587,
      "learning_rate": 1.0303703703703705e-05,
      "loss": 0.4977,
      "step": 1310
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.7993633151054382,
      "learning_rate": 1.0229629629629631e-05,
      "loss": 0.4678,
      "step": 1320
    },
    {
      "epoch": 1.4777777777777779,
      "grad_norm": 1.0051085948944092,
      "learning_rate": 1.0155555555555556e-05,
      "loss": 0.5202,
      "step": 1330
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 1.1819324493408203,
      "learning_rate": 1.0081481481481484e-05,
      "loss": 0.6154,
      "step": 1340
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.8674097061157227,
      "learning_rate": 1.0007407407407407e-05,
      "loss": 0.5859,
      "step": 1350
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 0.9558541178703308,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.5209,
      "step": 1360
    },
    {
      "epoch": 1.5222222222222221,
      "grad_norm": 1.2386484146118164,
      "learning_rate": 9.85925925925926e-06,
      "loss": 0.5944,
      "step": 1370
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.8166358470916748,
      "learning_rate": 9.785185185185187e-06,
      "loss": 0.4679,
      "step": 1380
    },
    {
      "epoch": 1.5444444444444443,
      "grad_norm": 0.8766558766365051,
      "learning_rate": 9.711111111111111e-06,
      "loss": 0.6375,
      "step": 1390
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 1.1753498315811157,
      "learning_rate": 9.637037037037038e-06,
      "loss": 0.5123,
      "step": 1400
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 1.3176172971725464,
      "learning_rate": 9.562962962962965e-06,
      "loss": 0.4941,
      "step": 1410
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 0.8532713651657104,
      "learning_rate": 9.48888888888889e-06,
      "loss": 0.5324,
      "step": 1420
    },
    {
      "epoch": 1.588888888888889,
      "grad_norm": 1.1958775520324707,
      "learning_rate": 9.414814814814816e-06,
      "loss": 0.6199,
      "step": 1430
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.5715198516845703,
      "learning_rate": 9.34074074074074e-06,
      "loss": 0.5086,
      "step": 1440
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 0.7353049516677856,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.5522,
      "step": 1450
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 0.573564350605011,
      "learning_rate": 9.192592592592594e-06,
      "loss": 0.4282,
      "step": 1460
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 1.1734492778778076,
      "learning_rate": 9.118518518518518e-06,
      "loss": 0.4995,
      "step": 1470
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 1.3983124494552612,
      "learning_rate": 9.044444444444445e-06,
      "loss": 0.5151,
      "step": 1480
    },
    {
      "epoch": 1.6555555555555554,
      "grad_norm": 0.9239808917045593,
      "learning_rate": 8.970370370370372e-06,
      "loss": 0.4744,
      "step": 1490
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.4731112718582153,
      "learning_rate": 8.896296296296298e-06,
      "loss": 0.5058,
      "step": 1500
    },
    {
      "epoch": 1.6777777777777778,
      "grad_norm": 1.3131517171859741,
      "learning_rate": 8.822222222222223e-06,
      "loss": 0.5821,
      "step": 1510
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 0.6984474658966064,
      "learning_rate": 8.74814814814815e-06,
      "loss": 0.5268,
      "step": 1520
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.8716662526130676,
      "learning_rate": 8.674074074074074e-06,
      "loss": 0.4946,
      "step": 1530
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 1.0896998643875122,
      "learning_rate": 8.6e-06,
      "loss": 0.5666,
      "step": 1540
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 1.0884398221969604,
      "learning_rate": 8.525925925925927e-06,
      "loss": 0.5332,
      "step": 1550
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.8238081932067871,
      "learning_rate": 8.451851851851852e-06,
      "loss": 0.5221,
      "step": 1560
    },
    {
      "epoch": 1.7444444444444445,
      "grad_norm": 0.8783182501792908,
      "learning_rate": 8.377777777777779e-06,
      "loss": 0.4779,
      "step": 1570
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 0.9326802492141724,
      "learning_rate": 8.303703703703705e-06,
      "loss": 0.4649,
      "step": 1580
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.7785916328430176,
      "learning_rate": 8.229629629629632e-06,
      "loss": 0.5498,
      "step": 1590
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.9934770464897156,
      "learning_rate": 8.155555555555556e-06,
      "loss": 0.5585,
      "step": 1600
    },
    {
      "epoch": 1.7888888888888888,
      "grad_norm": 0.7684066295623779,
      "learning_rate": 8.081481481481483e-06,
      "loss": 0.5323,
      "step": 1610
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.64108407497406,
      "learning_rate": 8.007407407407408e-06,
      "loss": 0.5544,
      "step": 1620
    },
    {
      "epoch": 1.8111111111111111,
      "grad_norm": 1.0710302591323853,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.7228,
      "step": 1630
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 0.754289984703064,
      "learning_rate": 7.859259259259259e-06,
      "loss": 0.5644,
      "step": 1640
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.7113538384437561,
      "learning_rate": 7.785185185185185e-06,
      "loss": 0.4512,
      "step": 1650
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 0.8683823347091675,
      "learning_rate": 7.711111111111112e-06,
      "loss": 0.5899,
      "step": 1660
    },
    {
      "epoch": 1.8555555555555556,
      "grad_norm": 1.1445657014846802,
      "learning_rate": 7.637037037037037e-06,
      "loss": 0.5575,
      "step": 1670
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.8116284608840942,
      "learning_rate": 7.562962962962963e-06,
      "loss": 0.4625,
      "step": 1680
    },
    {
      "epoch": 1.8777777777777778,
      "grad_norm": 0.8107958436012268,
      "learning_rate": 7.48888888888889e-06,
      "loss": 0.4778,
      "step": 1690
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 1.5301604270935059,
      "learning_rate": 7.4148148148148155e-06,
      "loss": 0.5266,
      "step": 1700
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.0229544639587402,
      "learning_rate": 7.340740740740742e-06,
      "loss": 0.4942,
      "step": 1710
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 0.6213555932044983,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.5132,
      "step": 1720
    },
    {
      "epoch": 1.9222222222222223,
      "grad_norm": 0.8947820067405701,
      "learning_rate": 7.192592592592593e-06,
      "loss": 0.4686,
      "step": 1730
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 1.260045051574707,
      "learning_rate": 7.118518518518519e-06,
      "loss": 0.5504,
      "step": 1740
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 0.9636734127998352,
      "learning_rate": 7.044444444444445e-06,
      "loss": 0.6419,
      "step": 1750
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 1.208499550819397,
      "learning_rate": 6.97037037037037e-06,
      "loss": 0.5223,
      "step": 1760
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.9701639413833618,
      "learning_rate": 6.896296296296297e-06,
      "loss": 0.6323,
      "step": 1770
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 1.0908957719802856,
      "learning_rate": 6.8222222222222225e-06,
      "loss": 0.5009,
      "step": 1780
    },
    {
      "epoch": 1.988888888888889,
      "grad_norm": 0.8868603110313416,
      "learning_rate": 6.748148148148149e-06,
      "loss": 0.561,
      "step": 1790
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.7654559016227722,
      "learning_rate": 6.674074074074075e-06,
      "loss": 0.6054,
      "step": 1800
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.48352134227752686,
      "eval_runtime": 8.5453,
      "eval_samples_per_second": 11.702,
      "eval_steps_per_second": 11.702,
      "step": 1800
    },
    {
      "epoch": 2.011111111111111,
      "grad_norm": 0.9610262513160706,
      "learning_rate": 6.600000000000001e-06,
      "loss": 0.4929,
      "step": 1810
    },
    {
      "epoch": 2.022222222222222,
      "grad_norm": 1.1256651878356934,
      "learning_rate": 6.525925925925927e-06,
      "loss": 0.4613,
      "step": 1820
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.7235560417175293,
      "learning_rate": 6.4518518518518525e-06,
      "loss": 0.4913,
      "step": 1830
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 0.7539188861846924,
      "learning_rate": 6.377777777777778e-06,
      "loss": 0.6138,
      "step": 1840
    },
    {
      "epoch": 2.0555555555555554,
      "grad_norm": 0.824923038482666,
      "learning_rate": 6.303703703703704e-06,
      "loss": 0.5633,
      "step": 1850
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 1.2485355138778687,
      "learning_rate": 6.2296296296296295e-06,
      "loss": 0.5097,
      "step": 1860
    },
    {
      "epoch": 2.077777777777778,
      "grad_norm": 0.8938132524490356,
      "learning_rate": 6.155555555555556e-06,
      "loss": 0.5257,
      "step": 1870
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 1.3626749515533447,
      "learning_rate": 6.081481481481482e-06,
      "loss": 0.5858,
      "step": 1880
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.1015167236328125,
      "learning_rate": 6.007407407407407e-06,
      "loss": 0.4968,
      "step": 1890
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 0.9197591543197632,
      "learning_rate": 5.933333333333335e-06,
      "loss": 0.5451,
      "step": 1900
    },
    {
      "epoch": 2.1222222222222222,
      "grad_norm": 1.3300108909606934,
      "learning_rate": 5.85925925925926e-06,
      "loss": 0.5131,
      "step": 1910
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 1.0302786827087402,
      "learning_rate": 5.785185185185186e-06,
      "loss": 0.4781,
      "step": 1920
    },
    {
      "epoch": 2.1444444444444444,
      "grad_norm": 0.8261989951133728,
      "learning_rate": 5.711111111111112e-06,
      "loss": 0.5095,
      "step": 1930
    },
    {
      "epoch": 2.1555555555555554,
      "grad_norm": 0.9339715838432312,
      "learning_rate": 5.637037037037037e-06,
      "loss": 0.4887,
      "step": 1940
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 1.1084119081497192,
      "learning_rate": 5.562962962962963e-06,
      "loss": 0.5112,
      "step": 1950
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 1.083862543106079,
      "learning_rate": 5.4888888888888895e-06,
      "loss": 0.5391,
      "step": 1960
    },
    {
      "epoch": 2.188888888888889,
      "grad_norm": 1.198646068572998,
      "learning_rate": 5.414814814814815e-06,
      "loss": 0.6198,
      "step": 1970
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.8007362484931946,
      "learning_rate": 5.340740740740741e-06,
      "loss": 0.5329,
      "step": 1980
    },
    {
      "epoch": 2.2111111111111112,
      "grad_norm": 1.068723440170288,
      "learning_rate": 5.2666666666666665e-06,
      "loss": 0.4996,
      "step": 1990
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 1.2795822620391846,
      "learning_rate": 5.192592592592594e-06,
      "loss": 0.4859,
      "step": 2000
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.9928513169288635,
      "learning_rate": 5.1185185185185195e-06,
      "loss": 0.5691,
      "step": 2010
    },
    {
      "epoch": 2.2444444444444445,
      "grad_norm": 1.0116643905639648,
      "learning_rate": 5.044444444444445e-06,
      "loss": 0.5101,
      "step": 2020
    },
    {
      "epoch": 2.2555555555555555,
      "grad_norm": 1.1221792697906494,
      "learning_rate": 4.970370370370371e-06,
      "loss": 0.5604,
      "step": 2030
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 1.0196725130081177,
      "learning_rate": 4.8962962962962965e-06,
      "loss": 0.5706,
      "step": 2040
    },
    {
      "epoch": 2.2777777777777777,
      "grad_norm": 0.9699794054031372,
      "learning_rate": 4.822222222222222e-06,
      "loss": 0.482,
      "step": 2050
    },
    {
      "epoch": 2.2888888888888888,
      "grad_norm": 0.9566560983657837,
      "learning_rate": 4.748148148148149e-06,
      "loss": 0.5574,
      "step": 2060
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.2003880739212036,
      "learning_rate": 4.674074074074074e-06,
      "loss": 0.6125,
      "step": 2070
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 0.9338127374649048,
      "learning_rate": 4.600000000000001e-06,
      "loss": 0.5071,
      "step": 2080
    },
    {
      "epoch": 2.3222222222222224,
      "grad_norm": 1.3474286794662476,
      "learning_rate": 4.5259259259259265e-06,
      "loss": 0.6059,
      "step": 2090
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 1.1896352767944336,
      "learning_rate": 4.451851851851852e-06,
      "loss": 0.5535,
      "step": 2100
    },
    {
      "epoch": 2.3444444444444446,
      "grad_norm": 0.9249967932701111,
      "learning_rate": 4.377777777777778e-06,
      "loss": 0.4431,
      "step": 2110
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 0.7237602472305298,
      "learning_rate": 4.3037037037037035e-06,
      "loss": 0.5222,
      "step": 2120
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 1.3881021738052368,
      "learning_rate": 4.22962962962963e-06,
      "loss": 0.4521,
      "step": 2130
    },
    {
      "epoch": 2.3777777777777778,
      "grad_norm": 0.9578656554222107,
      "learning_rate": 4.155555555555556e-06,
      "loss": 0.5366,
      "step": 2140
    },
    {
      "epoch": 2.388888888888889,
      "grad_norm": 0.7995139360427856,
      "learning_rate": 4.081481481481482e-06,
      "loss": 0.483,
      "step": 2150
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.9378472566604614,
      "learning_rate": 4.007407407407408e-06,
      "loss": 0.4714,
      "step": 2160
    },
    {
      "epoch": 2.411111111111111,
      "grad_norm": 1.2966145277023315,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.5984,
      "step": 2170
    },
    {
      "epoch": 2.422222222222222,
      "grad_norm": 0.9742760062217712,
      "learning_rate": 3.85925925925926e-06,
      "loss": 0.5261,
      "step": 2180
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 0.8642787337303162,
      "learning_rate": 3.7851851851851857e-06,
      "loss": 0.4538,
      "step": 2190
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.6800416111946106,
      "learning_rate": 3.7111111111111113e-06,
      "loss": 0.5365,
      "step": 2200
    },
    {
      "epoch": 2.4555555555555557,
      "grad_norm": 1.317986011505127,
      "learning_rate": 3.6370370370370374e-06,
      "loss": 0.5753,
      "step": 2210
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.9827696084976196,
      "learning_rate": 3.562962962962963e-06,
      "loss": 0.519,
      "step": 2220
    },
    {
      "epoch": 2.477777777777778,
      "grad_norm": 0.7549191117286682,
      "learning_rate": 3.4888888888888896e-06,
      "loss": 0.4981,
      "step": 2230
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 1.1606292724609375,
      "learning_rate": 3.4148148148148153e-06,
      "loss": 0.5244,
      "step": 2240
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.7367526292800903,
      "learning_rate": 3.340740740740741e-06,
      "loss": 0.5298,
      "step": 2250
    },
    {
      "epoch": 2.511111111111111,
      "grad_norm": 1.0130255222320557,
      "learning_rate": 3.266666666666667e-06,
      "loss": 0.4886,
      "step": 2260
    },
    {
      "epoch": 2.522222222222222,
      "grad_norm": 0.7256636619567871,
      "learning_rate": 3.1925925925925927e-06,
      "loss": 0.4981,
      "step": 2270
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 1.1477571725845337,
      "learning_rate": 3.1185185185185183e-06,
      "loss": 0.4807,
      "step": 2280
    },
    {
      "epoch": 2.5444444444444443,
      "grad_norm": 1.2083507776260376,
      "learning_rate": 3.044444444444445e-06,
      "loss": 0.5075,
      "step": 2290
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 0.7674494981765747,
      "learning_rate": 2.9703703703703705e-06,
      "loss": 0.5622,
      "step": 2300
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 0.9581993222236633,
      "learning_rate": 2.8962962962962966e-06,
      "loss": 0.5036,
      "step": 2310
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 1.2768950462341309,
      "learning_rate": 2.8222222222222223e-06,
      "loss": 0.4875,
      "step": 2320
    },
    {
      "epoch": 2.588888888888889,
      "grad_norm": 0.8843965530395508,
      "learning_rate": 2.748148148148148e-06,
      "loss": 0.5705,
      "step": 2330
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.9929459095001221,
      "learning_rate": 2.6740740740740744e-06,
      "loss": 0.4995,
      "step": 2340
    },
    {
      "epoch": 2.611111111111111,
      "grad_norm": 1.221327304840088,
      "learning_rate": 2.6e-06,
      "loss": 0.5615,
      "step": 2350
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 1.0569015741348267,
      "learning_rate": 2.525925925925926e-06,
      "loss": 0.6234,
      "step": 2360
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 1.1559571027755737,
      "learning_rate": 2.451851851851852e-06,
      "loss": 0.4635,
      "step": 2370
    },
    {
      "epoch": 2.6444444444444444,
      "grad_norm": 0.6408594846725464,
      "learning_rate": 2.377777777777778e-06,
      "loss": 0.5309,
      "step": 2380
    },
    {
      "epoch": 2.6555555555555554,
      "grad_norm": 0.6494415402412415,
      "learning_rate": 2.303703703703704e-06,
      "loss": 0.6329,
      "step": 2390
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.5705369710922241,
      "learning_rate": 2.22962962962963e-06,
      "loss": 0.4724,
      "step": 2400
    },
    {
      "epoch": 2.677777777777778,
      "grad_norm": 0.6379052996635437,
      "learning_rate": 2.1555555555555558e-06,
      "loss": 0.5353,
      "step": 2410
    },
    {
      "epoch": 2.688888888888889,
      "grad_norm": 1.0040814876556396,
      "learning_rate": 2.0814814814814814e-06,
      "loss": 0.5638,
      "step": 2420
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.9707983732223511,
      "learning_rate": 2.0074074074074075e-06,
      "loss": 0.5191,
      "step": 2430
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 0.8352236151695251,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.4976,
      "step": 2440
    },
    {
      "epoch": 2.7222222222222223,
      "grad_norm": 1.1219544410705566,
      "learning_rate": 1.8592592592592595e-06,
      "loss": 0.5185,
      "step": 2450
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 2.210869550704956,
      "learning_rate": 1.7851851851851853e-06,
      "loss": 0.472,
      "step": 2460
    },
    {
      "epoch": 2.7444444444444445,
      "grad_norm": 1.3275004625320435,
      "learning_rate": 1.7111111111111112e-06,
      "loss": 0.5781,
      "step": 2470
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 1.1966179609298706,
      "learning_rate": 1.6370370370370373e-06,
      "loss": 0.5362,
      "step": 2480
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 1.0135985612869263,
      "learning_rate": 1.562962962962963e-06,
      "loss": 0.5766,
      "step": 2490
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 1.0326130390167236,
      "learning_rate": 1.4888888888888888e-06,
      "loss": 0.6783,
      "step": 2500
    },
    {
      "epoch": 2.7888888888888888,
      "grad_norm": 0.9256129264831543,
      "learning_rate": 1.414814814814815e-06,
      "loss": 0.5699,
      "step": 2510
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.2369338274002075,
      "learning_rate": 1.3407407407407408e-06,
      "loss": 0.5607,
      "step": 2520
    },
    {
      "epoch": 2.811111111111111,
      "grad_norm": 0.7448363304138184,
      "learning_rate": 1.2666666666666669e-06,
      "loss": 0.4999,
      "step": 2530
    },
    {
      "epoch": 2.822222222222222,
      "grad_norm": 2.110478162765503,
      "learning_rate": 1.1925925925925928e-06,
      "loss": 0.5303,
      "step": 2540
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.7869073748588562,
      "learning_rate": 1.1185185185185186e-06,
      "loss": 0.4875,
      "step": 2550
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 0.8442164659500122,
      "learning_rate": 1.0444444444444445e-06,
      "loss": 0.461,
      "step": 2560
    },
    {
      "epoch": 2.8555555555555556,
      "grad_norm": 0.7585795521736145,
      "learning_rate": 9.703703703703704e-07,
      "loss": 0.6177,
      "step": 2570
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 1.5453344583511353,
      "learning_rate": 8.962962962962964e-07,
      "loss": 0.4985,
      "step": 2580
    },
    {
      "epoch": 2.8777777777777778,
      "grad_norm": 0.8744645714759827,
      "learning_rate": 8.222222222222223e-07,
      "loss": 0.5299,
      "step": 2590
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 1.5735410451889038,
      "learning_rate": 7.481481481481482e-07,
      "loss": 0.5165,
      "step": 2600
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.85732501745224,
      "learning_rate": 6.740740740740741e-07,
      "loss": 0.5686,
      "step": 2610
    },
    {
      "epoch": 2.911111111111111,
      "grad_norm": 0.7017801403999329,
      "learning_rate": 6.000000000000001e-07,
      "loss": 0.5374,
      "step": 2620
    },
    {
      "epoch": 2.9222222222222225,
      "grad_norm": 1.0194557905197144,
      "learning_rate": 5.25925925925926e-07,
      "loss": 0.5428,
      "step": 2630
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.7739719152450562,
      "learning_rate": 4.518518518518519e-07,
      "loss": 0.5412,
      "step": 2640
    },
    {
      "epoch": 2.9444444444444446,
      "grad_norm": 1.3280915021896362,
      "learning_rate": 3.777777777777778e-07,
      "loss": 0.5818,
      "step": 2650
    },
    {
      "epoch": 2.9555555555555557,
      "grad_norm": 1.678813099861145,
      "learning_rate": 3.0370370370370374e-07,
      "loss": 0.5692,
      "step": 2660
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 1.076317310333252,
      "learning_rate": 2.2962962962962964e-07,
      "loss": 0.6012,
      "step": 2670
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 0.9985448122024536,
      "learning_rate": 1.5555555555555556e-07,
      "loss": 0.5926,
      "step": 2680
    },
    {
      "epoch": 2.988888888888889,
      "grad_norm": 1.011489987373352,
      "learning_rate": 8.148148148148148e-08,
      "loss": 0.5546,
      "step": 2690
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.1907966136932373,
      "learning_rate": 7.407407407407408e-09,
      "loss": 0.6383,
      "step": 2700
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.4808470606803894,
      "eval_runtime": 7.8278,
      "eval_samples_per_second": 12.775,
      "eval_steps_per_second": 12.775,
      "step": 2700
    }
  ],
  "logging_steps": 10,
  "max_steps": 2700,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.225928245248e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
